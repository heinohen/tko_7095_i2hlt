# Tentti 2024-05-24 ans

## 1 What are embeddings

* Word embedding is a numerical representation of a word (in this context)
* For representation/embedding learning, we can also consider language models with two-sided context, since we do not benefit from causality (almost to the contrary)
* embeddings can be put in a vector which can be clustered. Words that somehow are related to eachother seem to get similar embeddings === close together in vector space. The similarity of two embeddings in the vector space done with "cosine similarity" or "eucledian distance" seems to correlate to our understanding how two words goes to each other with their meanings

## 2 Explain briefly what are label dependencies in context of sequence labeling tasks

For part-of-speech task it is important to capture and understand the context of the sequence. Label dependencies can provide help for this. For example it is very likely to see a pair of determiner + noun (DET, NOUN), determiner being ("a", "the") and noun for example "fox" or "elephant". But it is very UNlikely to see a pair of (DET, VERB) for example "a is" or "each is"

For named entity recognition task it is important to recongize right or legal sequences of IOB tags. For example (B-Person, I-Person) sequence is valid and legal, but (B-person, I-Organization) tagpair is invalid and should be very unlikely. This might indicate a mistake in the origin dataset.

## 3 Word2vec questions

word2vec embeddings primary use is as pretrained representations that can be used to initialize the embedding matrix of models for downstream tasks === transfer learning

* What does the;
  * input-layer: vocabulary size, one position for each word in a language, which has as many neurons as there are words in the vocabulary for training
  * hidden-layer: embeddings with layer size in terms of neurons is the dimensionality of the resulting word vectors of a certain word with length typically of 200-300. This is kept as _embedding matrix_ of the model
  * output-layer: has the same number of neurons as the input layer, outputlayer is discarded after the training of w2v
  stand for?

* self-supervised learning
* continuous bag of words approach:
  * target variable for network training is the middle word and the remaining _context words_ form the inputs so that the network is being trained to 'fill in the blank'
* skip-gram approach:
  * reverses the inputs and outputs so that the centre word is presented as the input and the target values are the context words

It turns out that the learned embedding space ofthen has an even richer semantic structure than just the proximity of related words, and that this allows for simple vector arithmetic. For example the concept that 'Paris is to France as Rome is to Italy' can be expressed through operatoins on the embedding vectors.

$v(Paris) - v(France) + v(Italy) ≃ v(Rome)$

* What task is word2vec used for:

Word2vec is a technique for creating vector representations of words. These words capture information about the meaning of the word based on the surrounding words. typical usecase is is with help of generated embeddingvectors:

* text classification, it can also classify text with not seen examples
* sentiment analysis, negative and positive words group together and get high similar scores
* Question answering: like what is the capital of France with examples from different capitals related to countries
* Recommendation system: based on preferences "similar items in bags by other customers"

* Word2vec embeddings are _STATIC_
* embeddings are "one per word" === river _bank_ and money _bank_ use the same embedding for word bank, hence losing the context
* embeddings are affected by the source corpora used to train the word2vec layer! THIS IS IMPORTANT

## 4 Causal language models

* Causal language models are used for text generation
* Estimate probability of next word given previous words _ONLY_, hence one-directional "left-to-right"

* Greedy decoding means that given a causal language mode it is straightforward to generate text.

1) init text to desired starting string (combination of words) or just start-of-text token
2) Select next word $w$ from $P(w | text)$ and append it to $text$
3) Repeat previous step as long as desired (max tokens or `</s>` sampled)

A naive selection strategy for step 2. is to always take the _most likely next word w_: $argmax_wP(w|text)$

But this is called _greedy decoding_ and it often gets trapped in _repetition loops_ and only produces "predictable" text

## 5 Bidirectional language models

* Estimates probability of a word given preceding and following words, tries to understand what goes in the missing hole in the text with context.
* Does not work well at language generation, as following text is also needed for context
* Works better at classification, sentiment analysis, named entity recognition, POS tagging, language translation, question-answering, virtual assistents
* BERT understands ambiguity in language as it understands the context, distinguishing with different meanings of the same word based on context
* MASKED language model

## 6 Tokenization - why is it usually done with ml

The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze. After tokenization the tokens themselves can be processed further NER POS

* Search engines. When you type a query into a search engine like Google, it employs tokenization to dissect your input.
* Machine translation. tokenize input, translate to required language and construct sentences
* Speech recognition, spoken words in to text, text is tokenized and processed further

## 7 Lemmatization

Lemmatization means to reduce words to base forms. Means groupuing together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma or dictionary form

Lemmatization is the algorithmic process to determine the lemma of a word based on its intended meaning.

Lemmatization seeks to distill words to their foundational forms. In this linguistic refinement the resultant base word is referred to as a "lemma"

* Reduces words to base form
* Link together different inflected forms of a word
* Can be analysed as a single item indentified by the word's lemma or dictionary form

## 8 IAA inter annotation agreement

Inter Annotation Agreement (IAA) is used to monitor annotation qualit and consistency

training data for a model can be noisy. this can be estimated with `cohens kappa statistic` which measures how often two annotators agree with eachother after accounting for the likehood that they agree by chance only

`Cohens kappa` = $\frac{P_o - P_e}{1-P_e}$

where $P_o$ is the accuracy

Cohens kappa value ranges from 0 to 1, with 1 being the highest score and 0 lowest

* Statistical measure
* Measures how well multiple annotators can make the same decision for a certain label or class
* Cohens kappa, 0 - 1

Usecases:

example sentiment analysis, multiple human annotators may be asked to label a set of tweets as expressing positive, negative or neutral sentiment. Inter-annotator agreement would be calculated to evaluate how consistently these annotators label the tweets

High agreement rate would suggest that the sentiment cat's are well-defined and there are not conflicts and that the annotators share an understaning what is ment by positive or negative sentiment tweet

## 9 IOB or BIO

* realm of sequence labeling identifying _token spans_ constituting mention of names locations organizations etc NER

Span start and extend typically marked using IOB tagging or (IOBES).

B == Begin of entity, I == inside of entity, O == outside of entity

example sentence "Matti Meikalainen likes to eat at Burger King"
| token | tag |
|--|--|
|Matti | B - person |
|Meikalainen| I - person|
| likes | O - outside|
| to | O - outside|
| eat | O - outside|
| at | O - outside|
|Burger| B - Organization|
|King| I - Organization|

Also has rules that other entity cannot start _INSIDE_ another entity, means that they are not legal actions and must not be allowed during training the model.

example usecases

IOB tagging can me used to mark any continuous non-overlapping spans of tokens and assign them in to categories

* Phrases
* Argumentative zones
* Semantic roles
* Hedged claims

All of the above can be generalized to NLP task TEXT ZONING

The method is unable to tag embedded tags inside other entities e.g. University of Turku is an entity of class ORGANIZATION but Turku is also entity of class LOCATION. Thats why many datasets use "longest span" method to limit the problem and just stick to the longst spans available and annotate that

## 10 Where can large amounts of raw text data can be collected

Raw text can be gathered from: _TENTTIKYSYMYS_

* Simply download from internet
* Publicly available datasets such
  * Common Crawl
  * Wikipedia
  * IMDB reviews
  * Book Corpuses such as: PROJEKTI LÖNNROT, project gutenberg
* Crawl internet yourself (be in mind with copyrights and trademarks)

problem going forward is data tainting through AI generated texts!

## 11 Inline annotations

Inline annotations are inserted directly in to the text as HTML or XML

sentence: Matti Meikalainen likes to eat at Burger King

`<Person>Matti Meikalainen </Person> likes to eat at <Organization> Burger King </Organization>`

Inline annotations can be easier to manage (one file instead of two) and edited and visualized using standard HTML / XML tools

## 12 standoff annotations

Standoff annotations are stored separately from the text; annotated parts of the text are indentified through character offsets using typical rules of programming substrings

Sentence: Matti Meikalainen likes to eat at Burger King

Annotation: `(0,17, Person) (34, 46, Organization)`

Standoff annotation is more expressive (e.g. no issue representing overlapping annotations) and generally used by dedicated annotation tools

## 13 Word "crummy"

* PRETRAINING with LANGUAGE MODELLING
* word2vec layer

The word "crummy" might be seen only once in the IMDB training data, not giving enough opportunity to learn it as a negative feature

the word2vec embeddings are trained on much larger textual corpus and the embedding for "crummy" is close to other negative words, which the classifier may benefit from

The embeddings encode a lot of information about the meaning of the words, even if they are rare or missing in the supervised training data which is often relatively small

To do it is to pretrain word2vec embedding layer with much larger corpora, inject that layer in your classifier and use the _exisiting_ information about the words as a starting point to your model training, and start learning on top of those and doing _task specific_ updating of the weights, hence already having a lot of more words that are semantically and syntaxically close to each other and fine-tuning the model with task specific information

## 14 multiclass multilabel binary classification

* Binary:
  * Only two possible outcomes 0 / 1 same as two possible classes for example "positive" / "negative", "spam" / "not spam"

  A special case of multiclass with class number == 2. Usually a simplification of the real problem, sentiment for example not usually 100% positive or negative, same for fake news

  Usecases:

  * Spam detection
  * Review (movies etc)
  * Fakenews detection
  * Churn prediction
  * Clickbait detection

* Multiclass:
  * Classes > 2, ONE SINGLE CLASSLABEL PREDICTED

  Usecases:

  * Topic categorization: assign exactly one label to a news document
  * Language identification - large web crawl of data, label documents with language identification
  * Handwriting recognition === image identification: identifying letters from handwriting aplhabets A,B,C etc as CLASSLABELS
  * Emotion detection - spectrum of emotions 8 - 10 and you want to decide what emotion this document represents
  * Product categorization
  * Customer segmentation
  * Document genre classification - news, advertisement e.g. for labels
  * Essay grading: predict a grade between 1 - 5 as CLASSLABELS
  * ChatGPT

* Multilabel
  * Multiple labels predicted from a larger base of labels
  
  Usecases:

  * Set of topics to a document from a larger topic category
  * MeSH Medical Subject Headings classify each biomedical publication with 10 - 15 terms
  * News article tagging
  * Medical condition prediction
  * Customer support ticket tagging

## 15 Word ambiguity & variety

### Ambiguity

#### Word level

* Different meanings for a single word eg. a tie
  * VERB: attach or fasten with string or similar cord
  * NOUN: a strip of material worn round the collar and tied in a knot
  * NOUN: a result in a game in which two or more competitors have the same score

#### Sentence level

* Kids make nutritious snacks: Are the kids making the snacks or used as snacks?
* She killed the man with the tie: Was the man wearing the tie or was it the murder weapon

### Variety

Same meaning can be expressed in many different ways. Same idea can be expressed in multiple different ways for example using different sentence structures or different words

* Much obliged -- thank you
* You're welcome -- you got it -- anything for you
* You don't get it -- you don't understand it
* Leave a message, i'll get back to you -- leave me a message and i will call you back
