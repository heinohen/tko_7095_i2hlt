# Tentti 2024-05-24 ans

## 1 What are embeddings

* Word embedding is a numerical representation of a word (in this context)
* For representation/embedding learning, we can also consider language models with two-sided context, since we do not benefit from causality (almost to the contrary)
* embeddings can be put in a vector which can be clustered. Words that somehow are related to eachother seem to get similar embeddings === close together in vector space. The similarity of two embeddings in the vector space done with "cosine similarity" or "eucledian distance" seems to correlate to our understanding how two words goes to each other with their meanings

## 2

## 3 Word2vec questions

* What does the;
  * input-layer: vocabulary size, one position for each word in a language, which has as many neurons as there are words in the vocabulary for training
  * hidden-layer: embeddings with layer size in terms of neurons is the dimensionality of the resulting word vectors of a certain word with length typically of 200-300. This is kept as _embedding matrix_ of the model
  * output-layer: has the same number of neurons as the input layer, outputlayer is discarded after the training of w2v
  stand for?

* self-supervised learning
* continuous bag of words approach:
  * target variable for network training is the middle word and the remaining _context words_ form the inputs sot that the network is being trained to 'fill in the blank'
* skip-gram approach:
  * reverses the inputs and outputs so that the centre word is presented as the input and the target values are the context words

It turns out that the learned embedding space ofthen has an even richer semantic structure than just the proximity of related words, and that this allows for simple vector arithmetic. For example the conept that 'Paris is to France as Rome is to Italy' can be expressed through operatoins on the embedding vectors.

$v(Paris) - v(France) + v(Italy) â‰ƒ v(Rome)$

* What task is word2vec used for:

Word2vec is a technique for creating vector representations of words. These words capture information about the meaning of the word based on the surrounding words. typical usecase is is with help of generated embeddingvectors:

* text classification, it can also classify text with not seen examples
* sentiment analysis, negative and positive words group together and get high similar scores
* Question answering: like what is the capital of France with examples from different capitals related to countries
* Recommendation system: based on preferences "similar items in bags by other customers"


## 4

## 5

## 6

## 7

## 8

## 9

## Word "crummy"

## multiclass multilabel binary classification
