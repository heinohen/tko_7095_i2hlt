# Tentti 2024-05-24 ans

## 1 What are embeddings

* Word embedding is a numerical representation of a word (in this context)
* For representation/embedding learning, we can also consider language models with two-sided context, since we do not benefit from causality (almost to the contrary)
* embeddings can be put in a vector which can be clustered. Words that somehow are related to eachother seem to get similar embeddings === close together in vector space. The similarity of two embeddings in the vector space done with "cosine similarity" or "eucledian distance" seems to correlate to our understanding how two words goes to each other with their meanings

## 2 Explain briefly what are label dependencies in context of sequence labeling tasks

## 3 Word2vec questions

* What does the;
  * input-layer: vocabulary size, one position for each word in a language, which has as many neurons as there are words in the vocabulary for training
  * hidden-layer: embeddings with layer size in terms of neurons is the dimensionality of the resulting word vectors of a certain word with length typically of 200-300. This is kept as _embedding matrix_ of the model
  * output-layer: has the same number of neurons as the input layer, outputlayer is discarded after the training of w2v
  stand for?

* self-supervised learning
* continuous bag of words approach:
  * target variable for network training is the middle word and the remaining _context words_ form the inputs sot that the network is being trained to 'fill in the blank'
* skip-gram approach:
  * reverses the inputs and outputs so that the centre word is presented as the input and the target values are the context words

It turns out that the learned embedding space ofthen has an even richer semantic structure than just the proximity of related words, and that this allows for simple vector arithmetic. For example the conept that 'Paris is to France as Rome is to Italy' can be expressed through operatoins on the embedding vectors.

$v(Paris) - v(France) + v(Italy) ≃ v(Rome)$

* What task is word2vec used for:

Word2vec is a technique for creating vector representations of words. These words capture information about the meaning of the word based on the surrounding words. typical usecase is is with help of generated embeddingvectors:

* text classification, it can also classify text with not seen examples
* sentiment analysis, negative and positive words group together and get high similar scores
* Question answering: like what is the capital of France with examples from different capitals related to countries
* Recommendation system: based on preferences "similar items in bags by other customers"

## 4 Causal language models

## 5 Tokenization - why is it usually done with ml

The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze. After tokenization the tokens themselves can be processed further NER POS

Usecase machine translation when input sentence is splitted into tokens and the tokens converted to desired language and then sentence is combined again

## 6 IAA inter annotation agreement

Inter annotation agreement is used to monitor annotation qualit and consistency

## 7 IOB or BIO

* realm of sequence labeling identifying _token spans_ constituting mention of names locations organizations etc NER

Span start and extend typically marked using IOB tagging or (IOBES).

B == Begin of entity, I == inside of entity, O == outside of entity

example sentence "Matti Meikalainen likes to eat at Burger King"

Matti  B - person
Meikalainen I - person
likes O - outside
to O - outside
eat O - outside
at O - outside
Burger B - Organization
King I - Organization

Also has rules that other entity cannot start _INSIDE_ another entity, means that they are not legal actions and must not be allowed during training the model.

example usecases

IOB tagging can me used to mark any continuous non-overlapping spans of tokens and assign them in to categories

* Phrases
* Argumentative zones
* Semantic roles
* Hedged claims

All of the above can be generalized to NLP task TEXT ZONING

The method is unable to tag embedded tags inside other entities e.g. University of Turku is an entity of class ORGANIZATION but Turku is also entity of class LOCATION. Thats why many datasets use "longest span" method to limit the problem and just stick to the longst spans available and annotate that

## 8 Where can large amounts of raw text data can be collected

Raw text can be gathered from: _TENTTIKYSYMYS_

* Simply download from internet
* Publicly available datasets such
  * Common Crawl
  * Wikipedia
  * IMDB reviews
  * Book Corpuses such as: PROJEKTI LÖNNROT, project gutenberg
* Crawl internet yourself (be in mind with copyrights and trademarks)

problem going forward is data tainting through AI generated texts!

## 9 Inline annotations

Inline annotations are inserted directly in to the text as HTML or XML

sentence: Matti Meikalainen likes to eat at Burger King

`<Person>Matti Meikalainen </Person> likes to eat at <Organization> Burger King </Organization>`

Inline annotations can be easier to manage (one file instead of two) and edited and visualized using standard HTML / XML tools

## 10 standoff annotations

Standoff annotations are stored separately from the text; annotated parts of the text are indentified through character offsets using typical rules of programming substrings

Sentence: Matti Meikalainen likes to eat at Burger King

Annotation: `(0,17, Person) (34, 46, Organization)`

Standoff annotation is more expressive (e.g. no issue representing overlapping annotations) and generally used by dedicated annotation tools

## Word "crummy"

## multiclass multilabel binary classification
