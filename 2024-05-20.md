# Tentti 2024-05-24 ans

## 1 What are embeddings

* Word embedding is a numerical representation of a word (in this context)
* For representation/embedding learning, we can also consider language models with two-sided context, since we do not benefit from causality (almost to the contrary)
* embeddings can be put in a vector which can be clustered. Words that somehow are related to eachother seem to get similar embeddings === close together in vector space. The similarity of two embeddings in the vector space done with "cosine similarity" or "eucledian distance" seems to correlate to our understanding how two words goes to each other with their meanings

## 2 Explain briefly what are label dependencies in context of sequence labeling tasks

## 3 Word2vec questions

* What does the;
  * input-layer: vocabulary size, one position for each word in a language, which has as many neurons as there are words in the vocabulary for training
  * hidden-layer: embeddings with layer size in terms of neurons is the dimensionality of the resulting word vectors of a certain word with length typically of 200-300. This is kept as _embedding matrix_ of the model
  * output-layer: has the same number of neurons as the input layer, outputlayer is discarded after the training of w2v
  stand for?

* self-supervised learning
* continuous bag of words approach:
  * target variable for network training is the middle word and the remaining _context words_ form the inputs sot that the network is being trained to 'fill in the blank'
* skip-gram approach:
  * reverses the inputs and outputs so that the centre word is presented as the input and the target values are the context words

It turns out that the learned embedding space ofthen has an even richer semantic structure than just the proximity of related words, and that this allows for simple vector arithmetic. For example the conept that 'Paris is to France as Rome is to Italy' can be expressed through operatoins on the embedding vectors.

$v(Paris) - v(France) + v(Italy) ≃ v(Rome)$

* What task is word2vec used for:

Word2vec is a technique for creating vector representations of words. These words capture information about the meaning of the word based on the surrounding words. typical usecase is is with help of generated embeddingvectors:

* text classification, it can also classify text with not seen examples
* sentiment analysis, negative and positive words group together and get high similar scores
* Question answering: like what is the capital of France with examples from different capitals related to countries
* Recommendation system: based on preferences "similar items in bags by other customers"

## 4 Causal language models

* Causal language models are used for text generation
* Estimate probability of next word given previous words _ONLY_, hence one-directional "left-to-right"

* Greedy decoding means that given a causal language mode it is straightforward to generate text.

1) init text to desired starting string (combination of words) or just start-of-text token
2) Select next word $w$ from $P(w | text)$ and append it to $text$
3) Repeat previous step as long as desired (max tokens or `</s>` sampled)

A naive selection strategy for step 2. is to always take the _most likely next word w_: $argmax_wP(w|text)$

But this is called _greedy decoding_ and it often gets trapped in _repetition loops_ and only produces "predictable" text

## 5 Tokenization - why is it usually done with ml

The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze. After tokenization the tokens themselves can be processed further NER POS

Usecase machine translation when input sentence is splitted into tokens and the tokens converted to desired language and then sentence is combined again

## 6 Lemmatization

Lemmatization means to reduce words to base forms. Means groupuing together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma or dictionary form

Lemmatization is the algorithmic process to determine the lemma of a word based on its intended meaning.

Lemmatization seeks to distill words to their foundational forms. In this linguistic refinement the resultant base word is referred to as a "lemma"

* Reduces words to base form
* Link together different inflected forms of a word
* Can be analysed as a single item indentified by the word's lemma or dictionary form

## 7 IAA inter annotation agreement

Inter Annotation Agreement (IAA) is used to monitor annotation qualit and consistency

## 7 IOB or BIO

* realm of sequence labeling identifying _token spans_ constituting mention of names locations organizations etc NER

Span start and extend typically marked using IOB tagging or (IOBES).

B == Begin of entity, I == inside of entity, O == outside of entity

example sentence "Matti Meikalainen likes to eat at Burger King"

Matti  B - person
Meikalainen I - person
likes O - outside
to O - outside
eat O - outside
at O - outside
Burger B - Organization
King I - Organization

Also has rules that other entity cannot start _INSIDE_ another entity, means that they are not legal actions and must not be allowed during training the model.

example usecases

IOB tagging can me used to mark any continuous non-overlapping spans of tokens and assign them in to categories

* Phrases
* Argumentative zones
* Semantic roles
* Hedged claims

All of the above can be generalized to NLP task TEXT ZONING

The method is unable to tag embedded tags inside other entities e.g. University of Turku is an entity of class ORGANIZATION but Turku is also entity of class LOCATION. Thats why many datasets use "longest span" method to limit the problem and just stick to the longst spans available and annotate that

## 8 Where can large amounts of raw text data can be collected

Raw text can be gathered from: _TENTTIKYSYMYS_

* Simply download from internet
* Publicly available datasets such
  * Common Crawl
  * Wikipedia
  * IMDB reviews
  * Book Corpuses such as: PROJEKTI LÖNNROT, project gutenberg
* Crawl internet yourself (be in mind with copyrights and trademarks)

problem going forward is data tainting through AI generated texts!

## 9 Inline annotations

Inline annotations are inserted directly in to the text as HTML or XML

sentence: Matti Meikalainen likes to eat at Burger King

`<Person>Matti Meikalainen </Person> likes to eat at <Organization> Burger King </Organization>`

Inline annotations can be easier to manage (one file instead of two) and edited and visualized using standard HTML / XML tools

## 10 standoff annotations

Standoff annotations are stored separately from the text; annotated parts of the text are indentified through character offsets using typical rules of programming substrings

Sentence: Matti Meikalainen likes to eat at Burger King

Annotation: `(0,17, Person) (34, 46, Organization)`

Standoff annotation is more expressive (e.g. no issue representing overlapping annotations) and generally used by dedicated annotation tools

## Word "crummy"

The word "crummy" might be seen only once in the IMDB training data, not giving enough opportunity to learn it as a negative feature

the word2vec embeddings are trained on much larger textual corpus and the embedding for "crummy" is close to other negative words, which the classifier may benefit from

## multiclass multilabel binary classification

* Binary:
  * Only two possible outcomes 0 / 1 same as two possible classes for example "positive" / "negative", "spam" / "not spam"

  A special case of multiclass with class number == 2. Usually a simplification of the real problem, sentiment for example not usually 100% positive or negative, same for fake news

  Usecases:

  * Spam detection
  * Review (movies etc)
  * Fakenews detection
  * Churn prediction
  * Clickbait detection

* Multiclass:
  * Classes > 2, ONE SINGLE CLASSLABEL PREDICTED

  Usecases:

  * Topic categorization: assign exactly one label to a news document
  * Language identification - large web crawl of data, label documents with language identification
  * Handwriting recognition === image identification: identifying letters from handwriting aplhabets A,B,C etc as CLASSLABELS
  * Emotion detection - spectrum of emotions 8 - 10 and you want to decide what emotion this document represents
  * Product categorization
  * Customer segmentation
  * Document genre classification - news, advertisement e.g. for labels
  * Essay grading: predict a grade between 1 - 5 as CLASSLABELS
  * ChatGPT

* Multilabel
  * Multiple labels predicted from a larger base of labels
  
  Usecases:

  * Set of topics to a document from a larger topic category
  * MeSH Medical Subject Headings classify each biomedical publication with 10 - 15 terms
  * News article tagging
  * Medical condition prediction
  * Customer support ticket tagging

## Word ambiguity & variety

### Ambiguity

#### Word level

* Different meanings for a single word eg. a tie
  * VERB: attach or fasten with string or similar cord
  * NOUN: a strip of material worn round the collar and tied in a knot
  * NOUN: a result in a game in which two or more competitors have the same score

#### Sentence level

* Kids make nutritious snacks: Are the kids making the snacks or used as snacks?
* She killed the man with the tie: Was the man wearing the tie or was it the murder weapon

### Variety

Same meaning can be expressed in many different ways. Same idea can be expressed in multiple different ways for example using different sentence structures or different words

* Much obliged -- thank you
* You're welcome -- you got it -- anything for you
* You don't get it -- you don't understand it
* Leave a message, i'll get back to you -- leave me a message and i will call you back
