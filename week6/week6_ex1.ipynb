{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNDHrmBKcXbJ9pMT9mjwWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heinohen/tko_7095_i2hlt/blob/main/week6_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 11: Top-K accuracy\n",
        "\n",
        "In the lecture, we learned to align embedding spaces, which allowed us to \"compute\" word translations. We marveled at how well this worked, but did not really evaluate this properly, even though we do have a nice test set.\n",
        "\n",
        "In the exercise, your task will be to evaluate the method using the simple \"top-k accuracy\" metric. This is a simple metric, which measures whether the correct target is among the first K nearest neighbors. In other words for the pair of source-target words\n",
        "we consider the transfer successful, if is among the K nearest neighbors of the embedding we obtain by transforming with the matrix . Top K accuracy then is the proportion of successfully transferred pairs, out of all pairs, as a percentage."
      ],
      "metadata": {
        "id": "b32fHpppEBFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VPXek7RSDnBG"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget http://vectors.nlpl.eu/repository/20/12.zip\n",
        "#!wget http://vectors.nlpl.eu/repository/20/42.zip 650 KB/s LOL\n",
        "\n",
        "## Try these if the download above is too slow, I mirrored these:\n",
        "!wget http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
        "!wget http://dl.turkunlp.org/TKO_7095_2023/42.zip # 22 MB/s, much better..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp296dxBFE-v",
        "outputId": "2b1f757b-ef5c-4573-efef-85b425f069f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 06:15:15--  http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 613577258 (585M) [application/zip]\n",
            "Saving to: ‘12.zip.1’\n",
            "\n",
            "12.zip.1            100%[===================>] 585.15M  15.2MB/s    in 40s     \n",
            "\n",
            "2024-04-20 06:15:55 (14.6 MB/s) - ‘12.zip.1’ saved [613577258/613577258]\n",
            "\n",
            "--2024-04-20 06:15:55--  http://dl.turkunlp.org/TKO_7095_2023/42.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1849124328 (1.7G) [application/zip]\n",
            "Saving to: ‘42.zip.1’\n",
            "\n",
            "42.zip.1            100%[===================>]   1.72G  15.6MB/s    in 2m 4s   \n",
            "\n",
            "2024-04-20 06:18:00 (14.2 MB/s) - ‘42.zip.1’ saved [1849124328/1849124328]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o 12.zip\n",
        "!mv model.bin en.bin\n",
        "!unzip -o 42.zip\n",
        "!mv model.bin fi.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7OqhZI8E4Ea",
        "outputId": "d9dda033-4682-4fd8-9510-a84c7ef7b746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  12.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n",
            "Archive:  42.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors # https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "fname == The file path to the saved word2vec-format file.\n",
        "limit == imit (int, optional) – Sets a maximum number of word-vectors to read from the file. The default, None, means read all.\n",
        "binary == binary (bool, optional) – If True, indicates whether the data is in binary word2vec format.\n",
        "\"\"\"\n",
        "\n",
        "wv_embeddings_en = KeyedVectors.load_word2vec_format(fname = 'en.bin', limit = 100000, binary = True)\n",
        "wv_embeddings_fi = KeyedVectors.load_word2vec_format(fname = 'fi.bin', limit = 100000, binary = True)"
      ],
      "metadata": {
        "id": "2sCdkNQDGIjk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/codogogo/xling-eval\n",
        "\n",
        "\n",
        "# Grab the data\n",
        "!wget https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.test.freq.2k.en-fi.tsv\n",
        "!wget https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.train.freq.5k.en-fi.tsv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFlCWyQ_HEPY",
        "outputId": "e1afee1d-d78e-4685-f7cd-00ba1965918d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 06:18:51--  https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.test.freq.2k.en-fi.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35770 (35K) [text/plain]\n",
            "Saving to: ‘yacle.test.freq.2k.en-fi.tsv.1’\n",
            "\n",
            "yacle.test.freq.2k. 100%[===================>]  34.93K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-04-20 06:18:51 (13.7 MB/s) - ‘yacle.test.freq.2k.en-fi.tsv.1’ saved [35770/35770]\n",
            "\n",
            "--2024-04-20 06:18:51--  https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.train.freq.5k.en-fi.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82957 (81K) [text/plain]\n",
            "Saving to: ‘yacle.train.freq.5k.en-fi.tsv.1’\n",
            "\n",
            "yacle.train.freq.5k 100%[===================>]  81.01K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-04-20 06:18:51 (8.85 MB/s) - ‘yacle.train.freq.5k.en-fi.tsv.1’ saved [82957/82957]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat yacle.test.freq.2k.en-fi.tsv | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omOCY3IsHQ9v",
        "outputId": "eee7da35-e98f-4a8a-bede-c98ddc463819"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dedication\tomistautuminen\n",
            "desires\ttoiveet\n",
            "dismissed\thylätty\n",
            "psychic\tpsyykkinen\n",
            "cracks\thalkeamia\n",
            "establishments\tlaitokset\n",
            "efficacy\ttehokkuus\n",
            "prestige\tarvovalta\n",
            "cocaine\tkokaiini\n",
            "accelerated\tkiihtyi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_train = [] #These will be pairs of (source,target) i.e. (Finnish, English) words used to induce the matrix M\n",
        "pairs_test = [] #same but for testing, so we should make sure there is absolutely no overlap between the train and test data\n",
        "               #let's do it so that not one word in the test is is seen in any capacity in the training data\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "def get_vectors(fname) -> list:\n",
        "  \"\"\"\n",
        "  Read the pairs from the file 'fname'\n",
        "  Returns: a list containing tuples with translation pairs\n",
        "  \"\"\"\n",
        "  pairs = []\n",
        "\n",
        "  with open(fname) as f:\n",
        "    r = csv.reader(f, delimiter = '\\t') # tab-serparated-values\n",
        "\n",
        "    for en_word, fi_word in r:\n",
        "      #I will reverse the order here, go from Finnish as the source, to English as the target\n",
        "      #That way it will be easier to check how this works using English as the target, which we all understand.\n",
        "      pairs.append((fi_word, en_word))\n",
        "  return pairs\n",
        "\n",
        "\n",
        "train_data = get_vectors('yacle.train.freq.5k.en-fi.tsv')\n",
        "test_data = get_vectors('yacle.test.freq.2k.en-fi.tsv')\n",
        "\n",
        "print(train_data[:10])\n",
        "print(len(train_data))\n",
        "\n",
        "print(test_data[:10])\n",
        "print(len(test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qOXjmRJHW9O",
        "outputId": "d49acea4-2907-4db4-aba8-c50b7bd61c6d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('of', 'of'), ('että', 'to'), ('sisään', 'in'), ('varten', 'for'), ('on', 'is'), ('päällä', 'on'), ('että', 'that'), ('mennessä', 'by'), ('Tämä', 'this'), ('kanssa', 'with')]\n",
            "5000\n",
            "[('omistautuminen', 'dedication'), ('toiveet', 'desires'), ('hylätty', 'dismissed'), ('psyykkinen', 'psychic'), ('halkeamia', 'cracks'), ('laitokset', 'establishments'), ('tehokkuus', 'efficacy'), ('arvovalta', 'prestige'), ('kokaiini', 'cocaine'), ('kiihtyi', 'accelerated')]\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the embeddings\n",
        "\n",
        "* Now we have the word pairs\n",
        "* We need the embeddings, so we can build our S and T matrices\n",
        "* Not all words will be in our W2V embeddings\n",
        "* Plus, we want to be 100% sure there is absolutely no overlap between the training and test data\n",
        "* This means not one word seen in the training data will be in the test data\n",
        "* The general approach will be to gather the vectors into a list, and then vstack (vertical stack) these to get a 2D array, i.e. a matrix"
      ],
      "metadata": {
        "id": "IZ8PGHLzI7nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_arrays(pairs, emb1, emb2, avoid = set()):\n",
        "  \"\"\"\n",
        "  Builds arrays for model to use\n",
        "  pairs == pairs of (fi,en) words\n",
        "  emb1 == source side (here Finnish) embeddings\n",
        "  emb2 == target side (here English) embeddings\n",
        "  avoid == a set of words to avoid or ignore (will be used when building test data, to avoid train data)\n",
        "  \"\"\"\n",
        "  source_vecs, target_vecs, filtered_pairs = [], [], []\n",
        "  for word1, word2 in pairs: # iterate through all pairs\n",
        "    # check if both vectors are available, and none of the words is to be avoided\n",
        "    if word1 in emb1 and word2 in emb2 and word1 not in avoid and word2 not in avoid:\n",
        "      # let's go\n",
        "      source_vecs.append(emb1[word1]) # source-side embedding, the KeyedVectors object can be queried as if it was a dict,\n",
        "                                      # returns the embedding as 1-dim array\n",
        "      target_vecs.append(emb2[word2])\n",
        "      filtered_pairs.append((word1,word2)) # remember the pair as tuple\n",
        "  return np.vstack(source_vecs),np.vstack(target_vecs),filtered_pairs\n",
        "\n",
        "\n",
        "# Gather the train data first\n",
        "array_train_fi, array_train_en, pairs_train = build_arrays(train_data, wv_embeddings_fi, wv_embeddings_en) # keep these in order!\n",
        "\n",
        "# Now build the set of all words seen in training, so we can avoid them when building the test set. Note that \"|\" is set union operator\n",
        "everything_in_train = set(s for s,t in pairs_train)|set(t for s,t in pairs_train)\n",
        "\n",
        "# Test data next, with avoid as the everything_in_train to ignore\n",
        "array_test_fi,array_test_en,pairs_test = build_arrays(test_data, wv_embeddings_fi, wv_embeddings_en, avoid = everything_in_train)"
      ],
      "metadata": {
        "id": "BTbuuUMBIycK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for absolutely no overlap\n",
        "\n",
        "# Let's be super-sure there absolutely is no overlap of any kind!\n",
        "print(\"Overlap between train pairs and test pairs:\",len(set(pairs_train) & set(pairs_test))) # & is set intersection operator, intersection between train and test should be empty\n",
        "src_train=set(src_w for src_w,tgt_w in pairs_train) #train source words\n",
        "tgt_train=set(tgt_w for src_w,tgt_w in pairs_train) #train target words\n",
        "src_test=set(src_w for src_w,tgt_w in pairs_test)   #test source words\n",
        "tgt_test=set(tgt_w for src_w,tgt_w in pairs_test)   #test target words\n",
        "print(\"Overlap between train fi words and test fi words:\",len(src_train & src_test))\n",
        "print(\"Overlap between train en words and test en words:\",len(tgt_train & tgt_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUYlgacIL-s8",
        "outputId": "d262281b-d074-4e69-dbb5-ca4aed4e5e39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap between train pairs and test pairs: 0\n",
            "Overlap between train fi words and test fi words: 0\n",
            "Overlap between train en words and test en words: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping matrix\n",
        "\n",
        "* Next we need to induce the transformation matrix\n",
        "* I.e implement the least-squares methods from the lecture\n",
        "* GPT4 for help"
      ],
      "metadata": {
        "id": "4PKKmNipMLFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was written by GPT4, but in a bit of a twisted form, so I modified it\n",
        "# to better correspond to the formulae in the lecture\n",
        "\n",
        "def learn_transformation_matrix(source, target):\n",
        "    # Compute the pseudo-inverse of the source matrix\n",
        "    source_pseudo_inverse = np.linalg.pinv(source) # This implements (S^T S)^-1 S^T  needed in the least-squares formula in the lecture slides\n",
        "    # Compute the transformation matrix M using least squares method\n",
        "    M = np.matmul(source_pseudo_inverse,target)  #...and this multiplies by T from right completing the formula in the slides ... two lines(!)\n",
        "    return M\n",
        "\n",
        "# fi -> en matrix\n",
        "M=learn_transformation_matrix(array_train_fi,array_train_en)\n",
        "\n"
      ],
      "metadata": {
        "id": "KjDm5RlTMc09"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Source (finnish) shape {array_train_fi.shape}')\n",
        "print(f'Target (english) shape {array_train_en.shape}')\n",
        "print(f'M shape {M.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbWo8VYnMvib",
        "outputId": "3814cbaa-2678-4aab-ce1c-8dd377d1b18c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source (finnish) shape (4506, 100)\n",
            "Target (english) shape (4506, 300)\n",
            "M shape (100, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And now we transform the source (finnish) test embeddings in to the english embedding space\n",
        "# using the matrix M\n",
        "\n",
        "test_fi_transformed = np.matmul(array_test_fi, M)\n",
        "print(f'Transformed shape: {test_fi_transformed.shape}')\n",
        "np.square(np.subtract(test_fi_transformed, array_test_en)).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRVkW23lNjlH",
        "outputId": "bbdf9343-f6c8-4c31-c846-e78e62ed36f6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed shape: (1285, 300)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002326297"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HOW TO EVALUATE\n",
        "\n",
        "1) Go over the test word pairs(fi,en)\n",
        "\n",
        "2) Use the transformed Finnish embedding as a query into the English space\n",
        "\n",
        "3) List top-N English words which appear near this transformed embedding"
      ],
      "metadata": {
        "id": "AN0rYiPtOVxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pairs_test))\n",
        "\n",
        "for i, (word1, word2) in enumerate(pairs_test[:10]):\n",
        "  print(f'{word1} --ENGLISH-> {word2}:')\n",
        "  \"\"\" SIMILAR BY VECTOR\n",
        "  Word2Vec.similar_by_vector(vector, topn=10, restrict_vocab=None)\n",
        "  \"\"\"\n",
        "  nn = wv_embeddings_en.similar_by_vector(test_fi_transformed[i]) # nearest neighbours\n",
        "  eng_words = [word for word, score in nn] # comes as tuples, need only words\n",
        "  print(f\"   \",\", \".join(eng_words)) #...and print then ,-separated\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxmj-A9aOSyl",
        "outputId": "9917c3c9-6ae4-4f28-c288-fae93648d950"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1285\n",
            "toiveet --ENGLISH-> desires:\n",
            "    desires, importantly, Certainly, qualities, ideas, perspectives, desire, indeed, sense, notions\n",
            "\n",
            "psyykkinen --ENGLISH-> psychic:\n",
            "    cognitive, physiological, behavioral, physical, neurological, mental, disorders, empathy, therapy, interpersonal\n",
            "\n",
            "halkeamia --ENGLISH-> cracks:\n",
            "    crevices, vegetation, gullies, surfaces, ridges, walls, limestone, reddish, mottled, sediment\n",
            "\n",
            "kokaiini --ENGLISH-> cocaine:\n",
            "    additives, pesticides, substances, caffeine, foods, carcinogenic, medications, drugs, side-effects, chemicals\n",
            "\n",
            "kiihtyi --ENGLISH-> accelerated:\n",
            "    slowed, worsened, accelerated, surged, exacerbated, spurred, stagnated, slackened, fueled, ebbed\n",
            "\n",
            "huippu --ENGLISH-> pinnacle:\n",
            "    magnificent, breathtaking, ideal, marvelous, majestic, perfect, beautiful, gorgeous, fabulous, awesome\n",
            "\n",
            "edellä --ENGLISH-> supra:\n",
            "    therefore, although, instances, indeed, simply, Furthermore, merely, Consequently, fact, actually\n",
            "\n",
            "päärynä --ENGLISH-> pear:\n",
            "    melon, tomato, salad, eggplant, pumpkin, peach, raspberry, cheese, roasted, chocolate\n",
            "\n",
            "törmäys --ENGLISH-> collision:\n",
            "    collisions, accident, collision, explosion, caused, malfunctioned, detected, derailment, fatal, leakage\n",
            "\n",
            "skandaali --ENGLISH-> scandal:\n",
            "    shame, embarrassment, revelation, backlash, outrage, irony, outraged, scandalous, accusation, betrayal\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build func for this so it can be run in sequence\n",
        "\n",
        "def nearest_top_k(value: int) -> float:\n",
        "  \"\"\"\n",
        "\n",
        "  Args:\n",
        "    value: how many nearest is included descending\n",
        "\n",
        "  Returns: percentage of accuracy as a float\n",
        "\n",
        "  \"\"\"\n",
        "  corr = 0 # within the top-K defined by value\n",
        "  all = 0 # all pairs\n",
        "  \"\"\" SIMILAR BY VECTOR\n",
        "  Word2Vec.similar_by_vector(vector, topn=10, restrict_vocab=None)\n",
        "  \"\"\"\n",
        "  # now use 'topn' attribute to limit the results\n",
        "  for i, (word1, word2) in enumerate(pairs_test):\n",
        "    nn = wv_embeddings_en.similar_by_vector(test_fi_transformed[i], topn = value)\n",
        "    # array means duplicates can appear, dict not needed as we will see only one instance, set is the way to go --> also fast because hashable\n",
        "    # store for calculations same from above code block, but instead of a array now a set\n",
        "    words = set(word for word, score in nn) # not intrested in the score of a individual word\n",
        "    # english word in pair resides in 'word2' variable so that is the target of inspection\n",
        "    if word2 in words: # if it is top-k defined by 'value', that means it is predicted correctly by criteria and we got a score\n",
        "      corr += 1\n",
        "    # if not we move on, but remember to update all\n",
        "    all += 1\n",
        "\n",
        "  # if i fu'd up\n",
        "  if all == 0:\n",
        "    return 0.0\n",
        "  # All done, return calculation\n",
        "  return 100 * float(corr) / float(all)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oCShj5wFRX1g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRINTS"
      ],
      "metadata": {
        "id": "kEp8RWYgU0xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tabulate\n",
        "\n",
        "one = nearest_top_k(1)\n",
        "five = nearest_top_k(5)\n",
        "ten = nearest_top_k(10)\n",
        "fifty = nearest_top_k(50)\n",
        "\n",
        "data = [\n",
        "    [\"1\", one],\n",
        "    [\"5\", five],\n",
        "    [\"10\", ten],\n",
        "    [\"50\", fifty]\n",
        "]\n",
        "\n",
        "print(tabulate.tabulate(data, headers=[\"TOP-K \", \"ACCURACY\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXdKVuKsUzyy",
        "outputId": "342ebd32-8182-43a1-f735-cb5dca390cd8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TOP-K     ACCURACY\n",
            "--------  ----------\n",
            "       1     17.821\n",
            "       5     34.0078\n",
            "      10     42.4125\n",
            "      50     61.0895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REFLECTION\n",
        "\n",
        "* Pretty cool task to learn!\n",
        "* Not that difficult as stub is given\n",
        "* Still time spent on task 3-4 hours to fully grasp the whole notebook"
      ],
      "metadata": {
        "id": "pmKBYJF5KeU0"
      }
    }
  ]
}