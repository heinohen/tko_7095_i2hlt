{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heinohen/tko_7095_i2hlt/blob/main/Blomqvist_Heinonen_course_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to HLT Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Mika Blomqvist & Henrik Heinonen\n",
        "- Date: 2024-05-02\n",
        "- Chosen Corpus: Rotten Tomatoes\n",
        "- Contributions (if group project):\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.\n",
        "- Paper(s) and other published materials related to the corpus:\n",
        "- State-of-the-art performance (best published results) on this corpus:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to install and import libraries etc. here\n",
        "!pip3 install -q transformers[torch] datasets evaluate optuna plotly\n",
        "!pip3 install -q datasets\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset_builder\n",
        "from datasets import load_dataset, DatasetDict\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        "from pprint import pprint # Pretty print\n",
        "import sklearn.feature_extraction\n"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d446789c-c0c4-4a3b-c121-ef18e42c1a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of rows : 10662 \n",
            "\n",
            "Relative sizes of subsets in the dataset: \n",
            "\n",
            "train: 80%\n",
            "validation: 10%\n",
            "test: 10%\n",
            "\n",
            "---\n",
            "\n",
            "Distribution of labels in the 'train' subset of the dataset: \n",
            "\n",
            "pos:50%\n",
            "neg:50%\n"
          ]
        }
      ],
      "source": [
        "# Your code to download the corpus here\n",
        "\n",
        "def dataset_features ( data : str ) -> DatasetDict:\n",
        "\n",
        "  dataset = datasets.load_dataset(data)\n",
        "  builder = datasets.load_dataset_builder(data)\n",
        "\n",
        "  print(builder.info.description)\n",
        "\n",
        "  osoittaja = 0\n",
        "  nimittaja = 0\n",
        "  tulos = 0\n",
        "\n",
        "  for rivi in dataset.keys():\n",
        "    nimittaja += dataset[rivi].num_rows\n",
        "\n",
        "  print(f\"Total number of rows : {nimittaja} \\n\")\n",
        "  print(\"Relative sizes of subsets in the dataset: \\n\")\n",
        "\n",
        "  for rivi in dataset.keys():\n",
        "    osoittaja = dataset[rivi].num_rows\n",
        "    tulos = osoittaja/nimittaja\n",
        "\n",
        "    print(f\"{rivi}: {tulos:.0%}\")\n",
        "\n",
        "\n",
        "  print(\"\\n---\\n\")\n",
        "  train_dataset = dataset['train']\n",
        "  label_names = train_dataset.features['label'].names\n",
        "  train_dict = {}\n",
        "\n",
        "  for indeksi in range(len(train_dataset)) :\n",
        "    label_name = label_names[train_dataset[indeksi]['label']]\n",
        "    if label_name not in train_dict :\n",
        "      train_dict[label_name] = 1\n",
        "    else:\n",
        "      train_dict[label_name] += 1\n",
        "\n",
        "  print(\"Distribution of labels in the 'train' subset of the dataset: \\n\")\n",
        "\n",
        "  for avain, arvo in train_dict.items():\n",
        "    tulos = arvo/len(train_dataset)\n",
        "    print(f\"{avain}:{tulos:.0%}\")\n",
        "\n",
        "  return dataset\n",
        "\n",
        "data = \"rotten_tomatoes\"\n",
        "\n",
        "dataset = dataset_features(data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for any necessary preprocessing here"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()"
      ],
      "metadata": {
        "id": "Dp5-gaQCxCIv"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset) # from assign 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozVhr1RBxCv4",
        "outputId": "64f4eb50-9efc-4dba-ac3b-9adaa2d81d51"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 8530\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1066\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 1066\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary = True, max_features = 20000)\n",
        "\n",
        "texts = [ex['text'] for ex in dataset['train']]\n",
        "vectorizer.fit(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HzZtTW4GxLRv",
        "outputId": "9d6412b0-bcce-4aad-bfa6-24f04e081658"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from course\n",
        "\n",
        "def vectorize_example(ex) -> dict:\n",
        "  vectorized = vectorizer.transform([ex['text']]) # Transform documents to document-term matrix.\n",
        "  non_zero_features = vectorized.nonzero()[1] # This is from torch 'nonzero' returns a 2-D tensor where each row is the index for a nonzero value.\n",
        "  non_zero_features += 1 #feature index 0 will have a special meaning\n",
        "                         # so let us not produce it by adding +1 to everything\n",
        "  return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized = vectorize_example(dataset['train'][0])"
      ],
      "metadata": {
        "id": "KXctyNaPxQjB"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "\n",
        "# Multiprocessing significantly speeds up processing by parallelizing processes on the CPU.\n",
        "# Set the num_proc parameter in map() to set the number of processes to use:\n",
        "\n",
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dset_tokenized = dataset.map(vectorize_example,num_proc=4)\n",
        "pprint(dset_tokenized[\"train\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIF9xSsSxVuB",
        "outputId": "5d3f75c8-96d5-4c7e-afa5-ae68284ea616"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [350,\n",
            "               556,\n",
            "               644,\n",
            "               662,\n",
            "               4264,\n",
            "               4600,\n",
            "               5022,\n",
            "               6288,\n",
            "               6385,\n",
            "               6712,\n",
            "               6942,\n",
            "               7815,\n",
            "               8819,\n",
            "               9801,\n",
            "               9993,\n",
            "               13061,\n",
            "               13158,\n",
            "               14637,\n",
            "               14692,\n",
            "               15800,\n",
            "               15997,\n",
            "               16227,\n",
            "               16405,\n",
            "               16417],\n",
            " 'label': 0,\n",
            " 'text': 'an admitted egomaniac , evans is no hollywood villain , and yet this '\n",
            "         \"grating showcase almost makes you wish he'd gone the way of don \"\n",
            "         'simpson .'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def collator(list_of_examples):\n",
        "  batch = {'labels':torch.tensor(list(ex['label'] for ex in list_of_examples))} # Labels in to a single tensor\n",
        "  tensors = []\n",
        "  max_len = max(len(example['input_ids']) for example in list_of_examples) # Get the length of longest input\n",
        "  # To build a tensor\n",
        "  for e in list_of_examples:\n",
        "    ids = torch.tensor(e['input_ids']) # Pick the input ids\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
        "    # pad(input, (left, right))\n",
        "    padded = torch.nn.functional.pad(ids, (0, max_len - ids.shape[0]))\n",
        "    tensors.append(padded)\n",
        "  # https://pytorch.org/docs/stable/generated/torch.vstack.html\n",
        "  batch['input_ids'] = torch.vstack(tensors) # Stack tensors in sequence vertically (row wise).\n",
        "  return batch"
      ],
      "metadata": {
        "id": "zpC2ruFkxoFo"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This is quite clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "        # Output layer: hidden size x output size\n",
        "        self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "\n",
        "    # The computation of the model is put into the forward() function\n",
        "    # it receives a batch of data and optionally the correct `labels`\n",
        "    #\n",
        "    # If given `labels` it returns (loss,output)\n",
        "    # if not, then it returns (output,)\n",
        "    def forward(self,input_ids,labels=None): #nevermind the attention_mask, its time will come, data collator insists on adding it\n",
        "        #1) sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "\n",
        "        #2) apply non-linearity\n",
        "        # (batch,embedding_dim) -> (batch,embedding_dim)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> commented out\n",
        "        ####projected=torch.tanh(embedded_summed) #Note how non-linearity is applied here and not when configuring the layer in __init__()\n",
        "\n",
        "        #3) and now apply the upper, output layer of the network\n",
        "        # (batch,embedding_dim) -> (batch, num_of_classes i.e. 2 in our case)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> base it off embedded_summed\n",
        "        ##### OLD: logits=self.output(projected)\n",
        "        logits=self.output(embedded_summed)\n",
        "\n",
        "        # ...and that's all there is to it!\n",
        "\n",
        "        #print(\"input_ids.shape\",input_ids.shape)\n",
        "        #print(\"embedded.shape\",embedded.shape)\n",
        "        #print(\"embedded_summed.shape\",embedded_summed.shape)\n",
        "        #print(\"projected.shape\",projected.shape)\n",
        "        #print(\"logits.shape\",logits.shape)\n",
        "\n",
        "        # We have labels, so we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)\n",
        "\n",
        "# Configure the model:\n",
        "#   these parameters are used in the model's __init__()\n",
        "\n",
        "\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=20,nlabels=2)\n"
      ],
      "metadata": {
        "id": "NtmLBHFWxvRD"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "mlp = MLP(mlp_config)\n",
        "fake_batch = collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call input_ids and labels as parameters to the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kPR2Z2nxy5S",
        "outputId": "0cf68cd6-72ff-487d-869f-2717bafffab8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.6662, grad_fn=<NllLossBackward0>),\n",
              " tensor([[-0.1366, -0.1987],\n",
              "         [-0.1443, -0.1915]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = 64\n",
        ")\n",
        "\n",
        "pprint(trainer_args) #print if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBl1j1W4x4AN",
        "outputId": "e42f72f4-cfea-46a5-d3f4-865a08b6083f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May02_21-11-17_2cf76f4d86dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Build more hyperparameter tests"
      ],
      "metadata": {
        "id": "RTMQp0XrGUY8"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = 64\n",
        ")\n",
        "\n",
        "pprint(trainer_args) #print if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGzYsjoAC4vn",
        "outputId": "9affd910-a03f-49df-a555-a7febf193caf"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May02_21-11-17_2cf76f4d86dd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Evaluate is a library that makes evaluating and comparing models\n",
        "# and reporting their performance easier and more standardized.\n",
        "# https://pypi.org/project/evaluate/\n",
        "\n",
        "accuracy = evaluate.load('accuracy')\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "  outputs, labels = outputs_and_labels\n",
        "  preds = np.argmax(outputs, axis = -1) # Returns the indices of the maximum values along an axis.\n",
        "  # https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
        "  return accuracy.compute(predictions = preds, references = labels)"
      ],
      "metadata": {
        "id": "F_eXaQuzDI5X"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=mlp,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dset_tokenized[\"train\"],\n",
        "    eval_dataset=dset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# FINALLY!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "n6XvevXoyQg8",
        "outputId": "7e0fc2fb-05b7-47eb-cad1-e103a05aee2c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5500' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5500/20000 01:06 < 02:55, 82.56 it/s, Epoch 41/150]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.644600</td>\n",
              "      <td>0.613596</td>\n",
              "      <td>0.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>0.536539</td>\n",
              "      <td>0.779000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.488547</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.319100</td>\n",
              "      <td>0.461678</td>\n",
              "      <td>0.799000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.258900</td>\n",
              "      <td>0.447899</td>\n",
              "      <td>0.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.214900</td>\n",
              "      <td>0.441159</td>\n",
              "      <td>0.793000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.180100</td>\n",
              "      <td>0.441746</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.152800</td>\n",
              "      <td>0.444968</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.130200</td>\n",
              "      <td>0.452422</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.112100</td>\n",
              "      <td>0.460901</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.096900</td>\n",
              "      <td>0.471686</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5500, training_loss=0.27473673664439807, metrics={'train_runtime': 66.715, 'train_samples_per_second': 19186.103, 'train_steps_per_second': 299.783, 'total_flos': 3046942584.0, 'train_loss': 0.27473673664439807, 'epoch': 41.04477611940298})"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the final model on the test set here\n",
        "\n",
        "eval_results = trainer.evaluate(dset_tokenized[\"test\"])\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "BG7s-yr6crGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ab4b5479-5bcd-4392-c200-1dea70434c41"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='134' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [134/134 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4417257308959961, 'eval_accuracy': 0.7926829268292683, 'eval_runtime': 0.2604, 'eval_samples_per_second': 4094.477, 'eval_steps_per_second': 514.69, 'epoch': 41.04477611940298}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "(Compare your results to the state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "(Briefly describe the chosen out-of-domain documents)\n",
        "\n",
        "*\n",
        "\n",
        "    TWEETS FROM https://github.com/MarkHershey/CompleteTrumpTweetsArchive\n",
        "    ANNOTATED BY HAND TEXT CLASSIFICATION 50 POS 50 NEG\n",
        "\n",
        "\n",
        "\n",
        "(Briefly describe the process of annotation)\n",
        "The dataset we used for annotation task consists of tweets by Donald Trump before and after inauguration. Because the needed size of annotated texts was small, only 100 tweets as individual documents, the process was pretty straight forward. The split of the data was 25 negatives and 25 positives from both before and after inauguration, totaling the 100 needed. For individual tweets we started looking for highly positive or negative words and after that tried to decide was it satire or not. Borderline cases included tweets that depend on which side of the political spectrum the reader resides in. Those were discarded in this small task. If the amount of data was larger, then we would have to reconsider. As for the test purposes we tried to select as positive or negative tweets as possible for our dataset. The annotation speed of the task was quick, because the size was small, and the tweets are very short documents. We found the contents of the tweets interesting, displaying polarity between the two timeframes. Also, the ethical side of the annotation process included reading a lot of hate speech which in large amounts can be harmful to the individual annotators’ mental well-being. We can only imagine what it feels like to do this for a living for a small monetary compensation.\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to convert the annotations into a dataset here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_pos.txt\n",
        "!wget https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_neg.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXIM7KHqwKXf",
        "outputId": "de8fbaa3-0489-45db-f205-f5c0e9c7cfff"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-02 21:12:26--  https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_pos.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7300 (7.1K) [text/plain]\n",
            "Saving to: ‘trump_pos.txt.1’\n",
            "\n",
            "trump_pos.txt.1     100%[===================>]   7.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-02 21:12:26 (78.8 MB/s) - ‘trump_pos.txt.1’ saved [7300/7300]\n",
            "\n",
            "--2024-05-02 21:12:26--  https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_neg.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7076 (6.9K) [text/plain]\n",
            "Saving to: ‘trump_neg.txt.1’\n",
            "\n",
            "trump_neg.txt.1     100%[===================>]   6.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-02 21:12:26 (84.3 MB/s) - ‘trump_neg.txt.1’ saved [7076/7076]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, ClassLabel, Features, Value, DatasetInfo"
      ],
      "metadata": {
        "id": "CEx2zOHTwOr9"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "with open('trump_pos.txt', 'r', encoding = 'utf-8') as f:\n",
        "  for row in f:\n",
        "    texts.append(row.strip())\n",
        "    labels.append(1)\n",
        "\n",
        "with open('trump_neg.txt', 'r', encoding = 'utf-8') as f:\n",
        "  for row in f:\n",
        "    texts.append(row.strip())\n",
        "    labels.append(0)\n",
        "\n",
        "# A special dictionary that defines the internal structure of a dataset\n",
        "features = Features({\n",
        "    'text': Value('string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['negative', 'positive'])\n",
        "})\n",
        "\n",
        "# The base class Dataset implements a Dataset backed by an Apache Arrow table.\n",
        "bonus_ds = Dataset.from_dict({'text': texts, 'label': labels}, features=features)\n",
        "\n",
        "# description (str) — A description of the dataset.\n",
        "bonus_ds.info.description = \"This dataset contains tweets from Donald Trump labeled as positive and negative. Annotation was done manually. Dataset has tweets from Donald Trump right before he was in office and after he was elected president. Ratio is 50/50.\"\n",
        "\n",
        "# Lets check that everything is ok\n",
        "for i in range(10):\n",
        "    print(ds[i]['text'], ds[i]['label'])\n",
        "\n",
        "# lets store the label names for further checking\n",
        "label_names = bonus_ds.features['label'].names\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Lets shuffle the database\n",
        "bonus_ds = bonus_ds.shuffle(seed=42)\n",
        "\n",
        "print(\"---------------------\")\n",
        "\n",
        "# And lets check the labeling once more\n",
        "for i in range(10):\n",
        "    example = bonus_ds[i]\n",
        "    text = example['text']\n",
        "    label_num = example['label']\n",
        "    label_name = bonus_ds.features['label'].int2str(label_num)\n",
        "    print(f\"Label: {label_num} ({label_name})\")\n",
        "    print('---')\n",
        "\n",
        "\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(bonus_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LbrvdUswUQ8",
        "outputId": "049a9ff3-1f34-4014-e066-a4e065f33b87"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016 was AMAZING, but we never had this kind of ENTHUSIASM! 1\n",
            "Will soon be heading to Wilmington, North Carolina, and then will be going to Battleship North Carolina. Look forward to seeing all of my friends! 1\n",
            "Mike has my complete &amp; total endorsement. We need him badly in Washington. A great fighter pilot &amp; hero, &amp; a brilliant Annapolis grad, Mike will never let you down. Mail in ballots, &amp; check that they are counted! 1\n",
            "I’m with the TRUCKERS all the way. Thanks for the meeting at the White House with my representatives from the Administration. It is all going to work out well! 1\n",
            "Congressman Bill Johnson (@JohnsonLeads) is an incredible fighter for the Great State of Ohio! He’s a proud Veteran and a hard worker who Cares for our Veterans, Supports Small Business, and is Strong on the Border and Second Amendment.... 1\n",
            "We are having very productive calls with the leaders of every sector of the economy who are all-in on getting America back to work, and soon. More to come! #MAGA 1\n",
            "A very good sign is that empty hospital beds are becoming more and more prevalent. We deployed 418 Doctors, Nurses and Respiratory Therapists from the hospital ship Comfort and the Javits Convention Center to hospitals in NYC &amp; State. Have more bed capacity than was needed. Good! 1\n",
            "America owes our very hard working food supply workers so much as they produce and deliver high quality food for us during this horrible COVID-19. Join me in thanking our Farmers, Ranchers, Processors, Distributors and Stores! @JohnBoozman 1\n",
            "GREAT news this week regarding the Keystone XL Pipeline – moving forward with fantastic paying CONSTRUCTION jobs for hardworking Americans. Promises Made, Promises Kept! #MAGA 1\n",
            "We are marshalling the full power of government and society to achieve victory over the virus. Together, we will endure, we will prevail, and we will WIN! #CARESAct 1\n",
            "---------------------\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "100\n",
            "100\n",
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_split = bonus_ds.train_test_split(test_size=0.9)\n",
        "test_valid_split = initial_split['test'].train_test_split(test_size=0.1)\n",
        "complete_dataset = DatasetDict({\n",
        "    'train': initial_split['train'],\n",
        "    'test': test_valid_split['train'],\n",
        "    'validate': test_valid_split['test']})\n",
        "\n",
        "print(complete_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTaEhdLmEx6K",
        "outputId": "fd4c48b3-e4a7-4ca3-dd9f-59d4b532aab7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 81\n",
            "    })\n",
            "    validate: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 9\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sklearn.feature_extraction\n",
        "\n",
        "# max_features means the size of the vocabulary\n",
        "# which means max_features most-common words\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True,max_features=20000)\n",
        "\n",
        "texts=[ex[\"text\"] for ex in complete_dataset[\"train\"]] #get a list of all texts from the training data\n",
        "vectorizer.fit(texts) #\"Trains\" the vectorizer, i.e. builds its vocabulary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "JUCR5WAOIlrz",
        "outputId": "90a44276-5c30-41bb-b98c-c2b16a74a44b"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bonus_vectorized=vectorize_example(complete_dataset[\"train\"][1])\n",
        "\n",
        "print(bonus_vectorized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBAlMYHSIr8Q",
        "outputId": "2982413c-4dad-4cab-f8c2-101a952e8681"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': array([ 10,  17,  20,  24,  27,  41,  64,  67,  68,  73,  76,  77,  84,\n",
            "        90,  94, 107, 113, 126, 128, 129, 133, 139, 144, 152, 158],\n",
            "      dtype=int32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "bonus_dset_tokenized = complete_dataset.map(vectorize_example,num_proc=4)\n",
        "\n",
        "#lets check one vector from the data\n",
        "example = bonus_dset_tokenized['train'][8]\n",
        "\n",
        "pprint(example)\n",
        "\n",
        "#Just checking that the labeling is still ok\n",
        "num_label = example['label']\n",
        "\n",
        "text_label = bonus_dset_tokenized['train'].features['label'].int2str(num_label)\n",
        "\n",
        "print(\"Numerical label:\", num_label)\n",
        "print(\"Corresponding text label:\", text_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK7UCo8XI0PX",
        "outputId": "6f844191-88f6-4cf5-b659-1ea8e498b2a2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [8, 14, 25, 37, 38, 44, 69, 104, 124, 133, 135, 146, 152, 156],\n",
            " 'label': 0,\n",
            " 'text': \"We could use the Balanced Budget Amendment--Politicians don't have \"\n",
            "         'the will to cut spending'}\n",
            "Numerical label: 0\n",
            "Corresponding text label: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(bonus_dset_tokenized[\"test\"])\n",
        "\n",
        "print(eval_results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "nEgMtqS6JHaU",
        "outputId": "58169136-d2c9-4b88-f3d6-5cbd991e5031"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='145' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [134/134 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.7451614737510681, 'eval_accuracy': 0.5432098765432098, 'eval_runtime': 0.0623, 'eval_samples_per_second': 1300.599, 'eval_steps_per_second': 176.625, 'epoch': 41.04477611940298}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "(Present the results of the evaluation on the out-of-domain test set)\n",
        "\n",
        "### 5.5. Annotated data"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your annotated out-of-domain data here"
      ],
      "metadata": {
        "id": "L2YJsiIGeYRe"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}