{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heinohen/tko_7095_i2hlt/blob/main/Blomqvist_Heinonen_course_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to HLT Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Mika Blomqvist & Henrik Heinonen\n",
        "- Date: 2024-05-02\n",
        "- Chosen Corpus: imdb\n",
        "- Contributions (if group project):\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
        "- Paper(s) and other published materials related to the corpus: https://aclanthology.org/P11-1015.pdf\n",
        "- State-of-the-art performance (best published results) on this corpus:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to install and import libraries etc. here\n",
        "!pip3 install -q transformers[torch] datasets evaluate plotly optuna\n",
        "!pip3 install -q datasets\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset_builder\n",
        "from datasets import load_dataset, DatasetDict\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        "from pprint import pprint # Pretty print\n",
        "import sklearn.feature_extraction\n"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c3ba39-85f3-4397-f6d5-22274b7d57f2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec5a244-a94d-4885-a61a-390210f45f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of rows : 100000 \n",
            "\n",
            "Relative sizes of subsets in the dataset: \n",
            "\n",
            "train: 25%\n",
            "test: 25%\n",
            "unsupervised: 50%\n",
            "\n",
            "---\n",
            "\n",
            "Distribution of labels in the 'train' subset of the dataset: \n",
            "\n",
            "neg:50%\n",
            "pos:50%\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Your code to download the corpus here\n",
        "\n",
        "def dataset_features ( data : str ) -> DatasetDict:\n",
        "\n",
        "  dataset = datasets.load_dataset(data)\n",
        "  builder = datasets.load_dataset_builder(data)\n",
        "\n",
        "  print(builder.info.description)\n",
        "\n",
        "  osoittaja = 0\n",
        "  nimittaja = 0\n",
        "  tulos = 0\n",
        "\n",
        "  for rivi in dataset.keys():\n",
        "    nimittaja += dataset[rivi].num_rows\n",
        "\n",
        "  print(f\"Total number of rows : {nimittaja} \\n\")\n",
        "  print(\"Relative sizes of subsets in the dataset: \\n\")\n",
        "\n",
        "  for rivi in dataset.keys():\n",
        "    osoittaja = dataset[rivi].num_rows\n",
        "    tulos = osoittaja/nimittaja\n",
        "\n",
        "    print(f\"{rivi}: {tulos:.0%}\")\n",
        "\n",
        "\n",
        "  print(\"\\n---\\n\")\n",
        "  train_dataset = dataset['train']\n",
        "  label_names = train_dataset.features['label'].names\n",
        "  train_dict = {}\n",
        "\n",
        "  for indeksi in range(len(train_dataset)) :\n",
        "    label_name = label_names[train_dataset[indeksi]['label']]\n",
        "    if label_name not in train_dict :\n",
        "      train_dict[label_name] = 1\n",
        "    else:\n",
        "      train_dict[label_name] += 1\n",
        "\n",
        "  print(\"Distribution of labels in the 'train' subset of the dataset: \\n\")\n",
        "\n",
        "  for avain, arvo in train_dict.items():\n",
        "    tulos = arvo/len(train_dataset)\n",
        "    print(f\"{avain}:{tulos:.0%}\")\n",
        "\n",
        "  return dataset\n",
        "\n",
        "data = \"imdb\"\n",
        "\n",
        "dataset = dataset_features(data)\n",
        "del dataset['unsupervised']\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_valid_split = dataset['test'].train_test_split(test_size=0.5)\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'],\n",
        "    'test': test_valid_split['train'],\n",
        "    'validate': test_valid_split['test']})\n",
        "\n",
        "print(dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyXcC5PpBK-Q",
        "outputId": "e8a6456b-48cb-4421-8057-858f2650ccb2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 12500\n",
            "    })\n",
            "    validate: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 12500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for any necessary preprocessing here"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()"
      ],
      "metadata": {
        "id": "Dp5-gaQCxCIv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary = True, max_features = 20000)\n",
        "\n",
        "texts = [ex['text'] for ex in dataset['train']]\n",
        "vectorizer.fit(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "HzZtTW4GxLRv",
        "outputId": "f4f838fa-9f89-4f53-dc26-97d9ca7db92c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from course\n",
        "\n",
        "def vectorize_example(ex) -> dict:\n",
        "  vectorized = vectorizer.transform([ex['text']]) # Transform documents to document-term matrix.\n",
        "  non_zero_features = vectorized.nonzero()[1] # This is from torch 'nonzero' returns a 2-D tensor where each row is the index for a nonzero value.\n",
        "  non_zero_features += 1 #feature index 0 will have a special meaning\n",
        "                         # so let us not produce it by adding +1 to everything\n",
        "  return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized = vectorize_example(dataset['train'][0])"
      ],
      "metadata": {
        "id": "KXctyNaPxQjB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "\n",
        "# Multiprocessing significantly speeds up processing by parallelizing processes on the CPU.\n",
        "# Set the num_proc parameter in map() to set the number of processes to use:\n",
        "\n",
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dset_tokenized = dataset.map(vectorize_example,num_proc=4)\n",
        "pprint(dset_tokenized[\"train\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIF9xSsSxVuB",
        "outputId": "12e3cbca-a608-4855-b87d-02f45f1958c1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [295,\n",
            "               309,\n",
            "               351,\n",
            "               774,\n",
            "               807,\n",
            "               860,\n",
            "               887,\n",
            "               959,\n",
            "               1157,\n",
            "               1744,\n",
            "               1756,\n",
            "               1793,\n",
            "               2103,\n",
            "               2257,\n",
            "               2295,\n",
            "               2604,\n",
            "               2625,\n",
            "               2924,\n",
            "               3053,\n",
            "               3317,\n",
            "               3603,\n",
            "               3669,\n",
            "               3671,\n",
            "               3672,\n",
            "               3797,\n",
            "               3950,\n",
            "               4257,\n",
            "               4906,\n",
            "               4924,\n",
            "               4963,\n",
            "               5540,\n",
            "               5559,\n",
            "               5736,\n",
            "               5896,\n",
            "               6306,\n",
            "               6308,\n",
            "               6584,\n",
            "               6904,\n",
            "               6907,\n",
            "               7082,\n",
            "               7127,\n",
            "               7327,\n",
            "               7330,\n",
            "               7394,\n",
            "               7590,\n",
            "               7757,\n",
            "               7967,\n",
            "               8128,\n",
            "               8289,\n",
            "               8322,\n",
            "               8350,\n",
            "               8476,\n",
            "               8526,\n",
            "               8567,\n",
            "               8592,\n",
            "               8602,\n",
            "               8710,\n",
            "               8855,\n",
            "               9047,\n",
            "               9058,\n",
            "               9085,\n",
            "               9256,\n",
            "               9602,\n",
            "               9628,\n",
            "               9630,\n",
            "               9997,\n",
            "               10029,\n",
            "               10324,\n",
            "               10449,\n",
            "               10551,\n",
            "               10625,\n",
            "               10696,\n",
            "               10829,\n",
            "               10915,\n",
            "               11189,\n",
            "               11297,\n",
            "               11419,\n",
            "               11681,\n",
            "               11762,\n",
            "               11832,\n",
            "               11833,\n",
            "               11836,\n",
            "               11856,\n",
            "               12000,\n",
            "               12202,\n",
            "               12316,\n",
            "               12363,\n",
            "               12439,\n",
            "               12564,\n",
            "               12627,\n",
            "               12692,\n",
            "               12892,\n",
            "               12898,\n",
            "               13028,\n",
            "               13063,\n",
            "               13285,\n",
            "               13937,\n",
            "               15101,\n",
            "               15255,\n",
            "               15835,\n",
            "               15892,\n",
            "               15919,\n",
            "               16516,\n",
            "               16888,\n",
            "               16902,\n",
            "               16951,\n",
            "               17122,\n",
            "               17639,\n",
            "               17893,\n",
            "               17897,\n",
            "               17907,\n",
            "               17938,\n",
            "               17944,\n",
            "               17952,\n",
            "               17968,\n",
            "               18074,\n",
            "               18115,\n",
            "               18123,\n",
            "               18130,\n",
            "               18474,\n",
            "               18484,\n",
            "               18553,\n",
            "               18685,\n",
            "               18823,\n",
            "               18922,\n",
            "               19398,\n",
            "               19404,\n",
            "               19521,\n",
            "               19537,\n",
            "               19551,\n",
            "               19559,\n",
            "               19562,\n",
            "               19590,\n",
            "               19712,\n",
            "               19715,\n",
            "               19800,\n",
            "               19914],\n",
            " 'label': 1,\n",
            " 'text': 'I was amazingly impressed by this movie. It contained fundamental '\n",
            "         'elements of depression, grief, loneliness, despair, hope, dreams and '\n",
            "         \"companionship. It wasn't merely about a genius musician who hit rock \"\n",
            "         'bottom but it was about a man caught up in grief trying drastically '\n",
            "         'to find solace within his music. He finds a companion who comes with '\n",
            "         'her own issues. Claire and Des were able to provide each other with '\n",
            "         'friendship and love but more importantly a conclusion to events '\n",
            "         'which had shaped their life for the worst. <br /><br />Des is an '\n",
            "         'unlikely character by todays standards of a rock star. Yet he has '\n",
            "         'musical genius. He also has an event in his past that has made him '\n",
            "         'stagnate, while things around him literally go to ruins. His focus '\n",
            "         'is creating his Whale Music, in fact it becomes an obsession for '\n",
            "         'him.<br /><br />Claire is the streetwise kid that needs a place to '\n",
            "         'stay. She finds hidden talents while being in Des company. She also '\n",
            "         'finds a mutual friend that accepts her. She learns to trust him over '\n",
            "         'a period of time.<br /><br />These two find love with one another. '\n",
            "         'Not the mind blowing, sex infused kind of passion, but a love where '\n",
            "         'friendship and understanding means more. For two people who have '\n",
            "         'been hurt, they find trust together.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def collator(list_of_examples):\n",
        "  batch = {'labels':torch.tensor(list(ex['label'] for ex in list_of_examples))} # Labels in to a single tensor\n",
        "  tensors = []\n",
        "  max_len = max(len(example['input_ids']) for example in list_of_examples) # Get the length of longest input\n",
        "  # To build a tensor\n",
        "  for e in list_of_examples:\n",
        "    ids = torch.tensor(e['input_ids']) # Pick the input ids\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
        "    # pad(input, (left, right))\n",
        "    padded = torch.nn.functional.pad(ids, (0, max_len - ids.shape[0]))\n",
        "    tensors.append(padded)\n",
        "  # https://pytorch.org/docs/stable/generated/torch.vstack.html\n",
        "  batch['input_ids'] = torch.vstack(tensors) # Stack tensors in sequence vertically (row wise).\n",
        "  return batch"
      ],
      "metadata": {
        "id": "zpC2ruFkxoFo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This is quite clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "        # Output layer: hidden size x output size\n",
        "        self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "\n",
        "    # The computation of the model is put into the forward() function\n",
        "    # it receives a batch of data and optionally the correct `labels`\n",
        "    #\n",
        "    # If given `labels` it returns (loss,output)\n",
        "    # if not, then it returns (output,)\n",
        "    def forward(self,input_ids,labels=None): #nevermind the attention_mask, its time will come, data collator insists on adding it\n",
        "        #1) sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "\n",
        "        #2) apply non-linearity\n",
        "        # (batch,embedding_dim) -> (batch,embedding_dim)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> commented out\n",
        "        ####projected=torch.tanh(embedded_summed) #Note how non-linearity is applied here and not when configuring the layer in __init__()\n",
        "\n",
        "        #3) and now apply the upper, output layer of the network\n",
        "        # (batch,embedding_dim) -> (batch, num_of_classes i.e. 2 in our case)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> base it off embedded_summed\n",
        "        ##### OLD: logits=self.output(projected)\n",
        "        logits=self.output(embedded_summed)\n",
        "\n",
        "        # ...and that's all there is to it!\n",
        "\n",
        "        #print(\"input_ids.shape\",input_ids.shape)\n",
        "        #print(\"embedded.shape\",embedded.shape)\n",
        "        #print(\"embedded_summed.shape\",embedded_summed.shape)\n",
        "        #print(\"projected.shape\",projected.shape)\n",
        "        #print(\"logits.shape\",logits.shape)\n",
        "\n",
        "        # We have labels, so we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)\n",
        "\n",
        "# Configure the model:\n",
        "#   these parameters are used in the model's __init__()\n",
        "\n",
        "\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=20,nlabels=2)\n"
      ],
      "metadata": {
        "id": "NtmLBHFWxvRD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "mlp = MLP(mlp_config)\n",
        "fake_batch = collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call input_ids and labels as parameters to the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kPR2Z2nxy5S",
        "outputId": "be64ac49-4d8b-4fe6-cb6a-4b64680fcae4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8185, grad_fn=<NllLossBackward0>),\n",
              " tensor([[ 0.2279, -0.0007],\n",
              "         [ 0.1343, -0.1105]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = 64\n",
        ")\n",
        "\n",
        "pprint(trainer_args) #print if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBl1j1W4x4AN",
        "outputId": "f55a2b95-e804-4641-87b0-30a78c4f3aaa"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May07_06-34-54_a7aa96911e3e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Build more hyperparameter tests"
      ],
      "metadata": {
        "id": "RTMQp0XrGUY8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = 64\n",
        ")\n",
        "\n",
        "pprint(trainer_args) #print if needed\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGzYsjoAC4vn",
        "outputId": "2d93cbe2-96e1-478e-c4d3-e112d946d73f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May07_06-34-54_a7aa96911e3e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Evaluate is a library that makes evaluating and comparing models\n",
        "# and reporting their performance easier and more standardized.\n",
        "# https://pypi.org/project/evaluate/\n",
        "\n",
        "accuracy = evaluate.load('accuracy')\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "  outputs, labels = outputs_and_labels\n",
        "  preds = np.argmax(outputs, axis = -1) # Returns the indices of the maximum values along an axis.\n",
        "  # https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
        "  return accuracy.compute(predictions = preds, references = labels)"
      ],
      "metadata": {
        "id": "F_eXaQuzDI5X"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=mlp,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dset_tokenized[\"train\"],\n",
        "    eval_dataset=dset_tokenized[\"validate\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# FINALLY!\n",
        "trainer.train()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "n6XvevXoyQg8",
        "outputId": "1d6f1291-50d4-43cb-ea22-7b78d63810a9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5000/20000 01:44 < 05:13, 47.89 it/s, Epoch 12/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.527300</td>\n",
              "      <td>0.418552</td>\n",
              "      <td>0.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.321500</td>\n",
              "      <td>0.321208</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.243200</td>\n",
              "      <td>0.291784</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>0.281546</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.168200</td>\n",
              "      <td>0.281514</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.143700</td>\n",
              "      <td>0.282806</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.128700</td>\n",
              "      <td>0.288915</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.109000</td>\n",
              "      <td>0.295761</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.307312</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.317462</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.2021428825378418, metrics={'train_runtime': 104.4017, 'train_samples_per_second': 12260.34, 'train_steps_per_second': 191.568, 'total_flos': 31773544992.0, 'train_loss': 0.2021428825378418, 'epoch': 12.787723785166241})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 64, 128, 256])\n",
        "\n",
        "\n",
        "    trainer_args = transformers.TrainingArguments(\n",
        "        \"mlp_checkpoints\", #save checkpoints here\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        logging_steps=500,\n",
        "        learning_rate=learning_rate, #learning rate of the gradient descent\n",
        "        max_steps=10000,\n",
        "        load_best_model_at_end=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    mlp = MLP(mlp_config)\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=mlp,\n",
        "        args=trainer_args,\n",
        "        train_dataset=dset_tokenized[\"train\"],\n",
        "        eval_dataset=dset_tokenized[\"validate\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "        compute_metrics=compute_accuracy,\n",
        "        data_collator=collator,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Train the model and get the best validation loss\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results[\"eval_accuracy\"] #let's try to maximize accuracy\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v84qLcD5y3mg",
        "outputId": "4171c1f2-010a-43dc-8ee1-331f66461a9d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:26:32,880] A new study created in memory with name: no-name-339b72d7-42d4-4ce6-b950-9269a9b5e4fe\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 03:15, Epoch 25/26]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.691334</td>\n",
              "      <td>0.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.688200</td>\n",
              "      <td>0.687066</td>\n",
              "      <td>0.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.684100</td>\n",
              "      <td>0.683344</td>\n",
              "      <td>0.548000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.680054</td>\n",
              "      <td>0.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.676500</td>\n",
              "      <td>0.677056</td>\n",
              "      <td>0.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.673100</td>\n",
              "      <td>0.674293</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.670400</td>\n",
              "      <td>0.671785</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.667400</td>\n",
              "      <td>0.669490</td>\n",
              "      <td>0.742000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.664900</td>\n",
              "      <td>0.667382</td>\n",
              "      <td>0.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.665471</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.660400</td>\n",
              "      <td>0.663739</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.658600</td>\n",
              "      <td>0.662197</td>\n",
              "      <td>0.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.657000</td>\n",
              "      <td>0.660833</td>\n",
              "      <td>0.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.655300</td>\n",
              "      <td>0.659656</td>\n",
              "      <td>0.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.654400</td>\n",
              "      <td>0.658664</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.652900</td>\n",
              "      <td>0.657846</td>\n",
              "      <td>0.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>0.657212</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.651700</td>\n",
              "      <td>0.656763</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.651400</td>\n",
              "      <td>0.656492</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>0.656402</td>\n",
              "      <td>0.795000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:29:48,999] Trial 0 finished with value: 0.795 and parameters: {'learning_rate': 1.066739746678008e-06, 'batch_size': 64}. Best is trial 0 with value: 0.795.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8000/10000 04:10 < 01:02, 31.93 it/s, Epoch 40/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.510022</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.421700</td>\n",
              "      <td>0.407560</td>\n",
              "      <td>0.862000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.333000</td>\n",
              "      <td>0.354180</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.323986</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.246600</td>\n",
              "      <td>0.306454</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.221900</td>\n",
              "      <td>0.295592</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.203700</td>\n",
              "      <td>0.288871</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.187400</td>\n",
              "      <td>0.284863</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.175900</td>\n",
              "      <td>0.282705</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.163700</td>\n",
              "      <td>0.281589</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.155900</td>\n",
              "      <td>0.280909</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.147400</td>\n",
              "      <td>0.281241</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.141600</td>\n",
              "      <td>0.281427</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.136000</td>\n",
              "      <td>0.282301</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.131600</td>\n",
              "      <td>0.283395</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.127300</td>\n",
              "      <td>0.283887</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:34:00,185] Trial 1 finished with value: 0.884 and parameters: {'learning_rate': 4.302032799816157e-05, 'batch_size': 128}. Best is trial 1 with value: 0.884.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 05:18, Epoch 51/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.670819</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.655500</td>\n",
              "      <td>0.649926</td>\n",
              "      <td>0.819000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.630131</td>\n",
              "      <td>0.828000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.609800</td>\n",
              "      <td>0.611733</td>\n",
              "      <td>0.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.589000</td>\n",
              "      <td>0.594849</td>\n",
              "      <td>0.839000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.570500</td>\n",
              "      <td>0.579553</td>\n",
              "      <td>0.839000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.554200</td>\n",
              "      <td>0.565878</td>\n",
              "      <td>0.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.539200</td>\n",
              "      <td>0.553609</td>\n",
              "      <td>0.841000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.526700</td>\n",
              "      <td>0.542758</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.514500</td>\n",
              "      <td>0.533108</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.504400</td>\n",
              "      <td>0.524633</td>\n",
              "      <td>0.847000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.495100</td>\n",
              "      <td>0.517265</td>\n",
              "      <td>0.845000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.487900</td>\n",
              "      <td>0.510942</td>\n",
              "      <td>0.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.481200</td>\n",
              "      <td>0.505573</td>\n",
              "      <td>0.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.475700</td>\n",
              "      <td>0.501135</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.470900</td>\n",
              "      <td>0.497536</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.467800</td>\n",
              "      <td>0.494768</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.464300</td>\n",
              "      <td>0.492808</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.462700</td>\n",
              "      <td>0.491633</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.462500</td>\n",
              "      <td>0.491245</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:39:19,263] Trial 2 finished with value: 0.851 and parameters: {'learning_rate': 5.434730358469891e-06, 'batch_size': 128}. Best is trial 1 with value: 0.884.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5000/10000 02:38 < 02:38, 31.56 it/s, Epoch 25/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.513300</td>\n",
              "      <td>0.411181</td>\n",
              "      <td>0.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.311900</td>\n",
              "      <td>0.323220</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.233700</td>\n",
              "      <td>0.295275</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.192400</td>\n",
              "      <td>0.284110</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.162900</td>\n",
              "      <td>0.281784</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.284918</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.125200</td>\n",
              "      <td>0.288333</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.110200</td>\n",
              "      <td>0.294786</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.099700</td>\n",
              "      <td>0.303686</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.088700</td>\n",
              "      <td>0.311257</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:41:58,344] Trial 3 finished with value: 0.885 and parameters: {'learning_rate': 8.379951734284147e-05, 'batch_size': 128}. Best is trial 3 with value: 0.885.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 05:18, Epoch 51/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.697147</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.691300</td>\n",
              "      <td>0.690751</td>\n",
              "      <td>0.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.684800</td>\n",
              "      <td>0.685565</td>\n",
              "      <td>0.519000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.679500</td>\n",
              "      <td>0.681044</td>\n",
              "      <td>0.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.674400</td>\n",
              "      <td>0.676966</td>\n",
              "      <td>0.558000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.669800</td>\n",
              "      <td>0.673243</td>\n",
              "      <td>0.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.665900</td>\n",
              "      <td>0.669846</td>\n",
              "      <td>0.601000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.661900</td>\n",
              "      <td>0.666705</td>\n",
              "      <td>0.615000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.658700</td>\n",
              "      <td>0.663847</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.655500</td>\n",
              "      <td>0.661231</td>\n",
              "      <td>0.646000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.652400</td>\n",
              "      <td>0.658867</td>\n",
              "      <td>0.651000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.649900</td>\n",
              "      <td>0.656756</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.647700</td>\n",
              "      <td>0.654911</td>\n",
              "      <td>0.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.645700</td>\n",
              "      <td>0.653312</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.643800</td>\n",
              "      <td>0.651963</td>\n",
              "      <td>0.681000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.642700</td>\n",
              "      <td>0.650858</td>\n",
              "      <td>0.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.641600</td>\n",
              "      <td>0.649999</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.640400</td>\n",
              "      <td>0.649385</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.649016</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.648894</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-07 07:47:17,366] Trial 4 finished with value: 0.687 and parameters: {'learning_rate': 1.172646301517167e-06, 'batch_size': 128}. Best is trial 3 with value: 0.885.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn = study.best_params['learning_rate']\n",
        "\n",
        "batchsize = study.best_params['batch_size']"
      ],
      "metadata": {
        "id": "hOYLShaT5vV2"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(learn, batchsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeAMmHzi58Ib",
        "outputId": "a98bb602-a8d6-471a-a7f6-a4160c9083ec"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.379951734284147e-05 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "vali = MLP(mlp_config)\n",
        "fake_batch = collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call input_ids and labels as parameters to the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZSWffnCBtWX",
        "outputId": "3fb43b46-c17a-4822-acbd-d0f0f89786a6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0448, grad_fn=<NllLossBackward0>),\n",
              " tensor([[-0.8853,  1.4817],\n",
              "         [-5.7466,  7.7066]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "vali_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=learn, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = batchsize\n",
        ")\n",
        "\n",
        "pprint(vali_args) #print if needed\n",
        "\n",
        "vali_trainer = transformers.Trainer(\n",
        "    model = vali,\n",
        "    args = vali_args,\n",
        "    train_dataset = dset_tokenized['train'],\n",
        "    eval_dataset = dset_tokenized['test'].select(range(1000)),\n",
        "    compute_metrics = compute_accuracy,\n",
        "    data_collator = collator,\n",
        "    callbacks = [early_stopping]\n",
        ")\n",
        "\n",
        "vali_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "imEugvq3B1qm",
        "outputId": "e4d30bb9-7682-4f4f-cb49-c331402a0e83"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=8.379951734284147e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May07_07-50-52_a7aa96911e3e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3000/20000 02:18 < 13:02, 21.72 it/s, Epoch 15/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.141300</td>\n",
              "      <td>0.236634</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.116400</td>\n",
              "      <td>0.241385</td>\n",
              "      <td>0.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.096500</td>\n",
              "      <td>0.250631</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.081100</td>\n",
              "      <td>0.266354</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.068100</td>\n",
              "      <td>0.284690</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>0.294407</td>\n",
              "      <td>0.891000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3000, training_loss=0.09346104049682617, metrics={'train_runtime': 138.095, 'train_samples_per_second': 18537.965, 'train_steps_per_second': 144.828, 'total_flos': 41158147968.0, 'train_loss': 0.09346104049682617, 'epoch': 15.306122448979592})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vali_trainer.predict(dset_tokenized['test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "xXU11w0ZCB27",
        "outputId": "de592d29-8e5a-4db7-b82d-03ff103a2d95"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-0.06904455,  0.2582905 ],\n",
              "       [-3.7400405 ,  4.6512074 ],\n",
              "       [-0.77783036,  1.1135826 ],\n",
              "       ...,\n",
              "       [ 1.1818006 , -1.2326403 ],\n",
              "       [ 2.9604218 , -3.367769  ],\n",
              "       [-1.2348528 ,  1.6630334 ]], dtype=float32), label_ids=array([0, 1, 1, ..., 0, 0, 1]), metrics={'test_loss': 0.28368690609931946, 'test_accuracy': 0.88816, 'test_runtime': 4.1213, 'test_samples_per_second': 3033.007, 'test_steps_per_second': 379.247})"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "bonus = MLP(mlp_config)\n",
        "fake_batch = collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call input_ids and labels as parameters to the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQYkt5M_6S1c",
        "outputId": "df840035-ae04-4165-f4b9-982a43205f5a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0448, grad_fn=<NllLossBackward0>),\n",
              " tensor([[-0.8853,  1.4817],\n",
              "         [-5.7466,  7.7066]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "bonus_trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=learn, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) — The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = batchsize\n",
        ")\n",
        "\n",
        "pprint(bonus_trainer_args) #print if needed\n",
        "\n",
        "bonus_trainer = transformers.Trainer(\n",
        "    model = bonus,\n",
        "    args = bonus_trainer_args,\n",
        "    train_dataset = dset_tokenized['train'],\n",
        "    eval_dataset = dset_tokenized['test'].select(range(1000)),\n",
        "    compute_metrics = compute_accuracy,\n",
        "    data_collator = collator,\n",
        "    callbacks = [early_stopping]\n",
        ")\n",
        "\n",
        "bonus_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VuqieM6R6CbH",
        "outputId": "f7665435-e3cd-4f9d-e4ca-6e0d338ad340"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=8.379951734284147e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May07_07-54-26_a7aa96911e3e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3000/20000 01:42 < 09:40, 29.31 it/s, Epoch 15/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.119900</td>\n",
              "      <td>0.243638</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.104300</td>\n",
              "      <td>0.251385</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.083800</td>\n",
              "      <td>0.262608</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.070200</td>\n",
              "      <td>0.280113</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.058700</td>\n",
              "      <td>0.299319</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.049300</td>\n",
              "      <td>0.310243</td>\n",
              "      <td>0.890000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3000, training_loss=0.08103721110026042, metrics={'train_runtime': 102.342, 'train_samples_per_second': 25014.174, 'train_steps_per_second': 195.423, 'total_flos': 41158147968.0, 'train_loss': 0.08103721110026042, 'epoch': 15.306122448979592})"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "(Compare your results to the state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "(Briefly describe the chosen out-of-domain documents)\n",
        "\n",
        "*\n",
        "\n",
        "    TWEETS FROM https://github.com/MarkHershey/CompleteTrumpTweetsArchive\n",
        "    ANNOTATED BY HAND TEXT CLASSIFICATION 50 POS 50 NEG\n",
        "\n",
        "\n",
        "\n",
        "(Briefly describe the process of annotation)\n",
        "The dataset we used for annotation task consists of tweets by Donald Trump before and after inauguration. Because the needed size of annotated texts was small, only 100 tweets as individual documents, the process was pretty straight forward. The split of the data was 25 negatives and 25 positives from both before and after inauguration, totaling the 100 needed. For individual tweets we started looking for highly positive or negative words and after that tried to decide was it satire or not. Borderline cases included tweets that depend on which side of the political spectrum the reader resides in. Those were discarded in this small task. If the amount of data was larger, then we would have to reconsider. As for the test purposes we tried to select as positive or negative tweets as possible for our dataset. The annotation speed of the task was quick, because the size was small, and the tweets are very short documents. We found the contents of the tweets interesting, displaying polarity between the two timeframes. Also, the ethical side of the annotation process included reading a lot of hate speech which in large amounts can be harmful to the individual annotators’ mental well-being. We can only imagine what it feels like to do this for a living for a small monetary compensation.\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to convert the annotations into a dataset here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_pos.txt\n",
        "!wget https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_neg.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXIM7KHqwKXf",
        "outputId": "404bcb7e-c4f5-4cc3-c7d2-de99479a8767"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-07 07:56:08--  https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_pos.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7300 (7.1K) [text/plain]\n",
            "Saving to: ‘trump_pos.txt.2’\n",
            "\n",
            "trump_pos.txt.2     100%[===================>]   7.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-07 07:56:09 (37.5 MB/s) - ‘trump_pos.txt.2’ saved [7300/7300]\n",
            "\n",
            "--2024-05-07 07:56:09--  https://raw.githubusercontent.com/heinohen/tko_7095_i2hlt/main/prjct/trump_neg.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7076 (6.9K) [text/plain]\n",
            "Saving to: ‘trump_neg.txt.2’\n",
            "\n",
            "trump_neg.txt.2     100%[===================>]   6.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-07 07:56:09 (62.3 MB/s) - ‘trump_neg.txt.2’ saved [7076/7076]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, ClassLabel, Features, Value, DatasetInfo"
      ],
      "metadata": {
        "id": "CEx2zOHTwOr9"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "with open('trump_pos.txt', 'r', encoding = 'utf-8') as f:\n",
        "  for row in f:\n",
        "    texts.append(row.strip())\n",
        "    labels.append(1)\n",
        "\n",
        "with open('trump_neg.txt', 'r', encoding = 'utf-8') as f:\n",
        "  for row in f:\n",
        "    texts.append(row.strip())\n",
        "    labels.append(0)\n",
        "\n",
        "# A special dictionary that defines the internal structure of a dataset\n",
        "features = Features({\n",
        "    'text': Value('string'),\n",
        "    'label': ClassLabel(num_classes=2, names=['negative', 'positive'])\n",
        "})\n",
        "\n",
        "# The base class Dataset implements a Dataset backed by an Apache Arrow table.\n",
        "bonus_ds = Dataset.from_dict({'text': texts, 'label': labels}, features=features)\n",
        "\n",
        "# description (str) — A description of the dataset.\n",
        "bonus_ds.info.description = \"This dataset contains tweets from Donald Trump labeled as positive and negative. Annotation was done manually. Dataset has tweets from Donald Trump right before he was in office and after he was elected president. Ratio is 50/50.\"\n",
        "\n",
        "# Lets check that everything is ok\n",
        "for i in range(10):\n",
        "    print(bonus_ds[i]['text'], bonus_ds[i]['label'])\n",
        "\n",
        "# lets store the label names for further checking\n",
        "label_names = bonus_ds.features['label'].names\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Lets shuffle the database\n",
        "bonus_ds = bonus_ds.shuffle(seed=42)\n",
        "\n",
        "print(\"---------------------\")\n",
        "\n",
        "# And lets check the labeling once more\n",
        "for i in range(10):\n",
        "    example = bonus_ds[i]\n",
        "    text = example['text']\n",
        "    label_num = example['label']\n",
        "    label_name = bonus_ds.features['label'].int2str(label_num)\n",
        "    print(f\"Label: {label_num} ({label_name})\")\n",
        "    print('---')\n",
        "\n",
        "\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(bonus_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LbrvdUswUQ8",
        "outputId": "2da39392-d9d3-4952-c8e6-bd7c7cd77134"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016 was AMAZING, but we never had this kind of ENTHUSIASM! 1\n",
            "Will soon be heading to Wilmington, North Carolina, and then will be going to Battleship North Carolina. Look forward to seeing all of my friends! 1\n",
            "Mike has my complete &amp; total endorsement. We need him badly in Washington. A great fighter pilot &amp; hero, &amp; a brilliant Annapolis grad, Mike will never let you down. Mail in ballots, &amp; check that they are counted! 1\n",
            "I’m with the TRUCKERS all the way. Thanks for the meeting at the White House with my representatives from the Administration. It is all going to work out well! 1\n",
            "Congressman Bill Johnson (@JohnsonLeads) is an incredible fighter for the Great State of Ohio! He’s a proud Veteran and a hard worker who Cares for our Veterans, Supports Small Business, and is Strong on the Border and Second Amendment.... 1\n",
            "We are having very productive calls with the leaders of every sector of the economy who are all-in on getting America back to work, and soon. More to come! #MAGA 1\n",
            "A very good sign is that empty hospital beds are becoming more and more prevalent. We deployed 418 Doctors, Nurses and Respiratory Therapists from the hospital ship Comfort and the Javits Convention Center to hospitals in NYC &amp; State. Have more bed capacity than was needed. Good! 1\n",
            "America owes our very hard working food supply workers so much as they produce and deliver high quality food for us during this horrible COVID-19. Join me in thanking our Farmers, Ranchers, Processors, Distributors and Stores! @JohnBoozman 1\n",
            "GREAT news this week regarding the Keystone XL Pipeline – moving forward with fantastic paying CONSTRUCTION jobs for hardworking Americans. Promises Made, Promises Kept! #MAGA 1\n",
            "We are marshalling the full power of government and society to achieve victory over the virus. Together, we will endure, we will prevail, and we will WIN! #CARESAct 1\n",
            "---------------------\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Label: 0 (negative)\n",
            "---\n",
            "100\n",
            "100\n",
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sklearn.feature_extraction\n",
        "\n",
        "# max_features means the size of the vocabulary\n",
        "# which means max_features most-common words\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True,max_features=20000)\n",
        "\n",
        "texts=[ex[\"text\"] for ex in bonus_ds] #get a list of all texts from the training data ['train']\n",
        "vectorizer.fit(texts) #\"Trains\" the vectorizer, i.e. builds its vocabulary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "JUCR5WAOIlrz",
        "outputId": "188423d9-73e3-4203-f3c7-f51d2e4209a7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMV7vChw9G74"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bonus_vectorized=vectorize_example(bonus_ds[1]) #['train']\n",
        "\n",
        "print(bonus_vectorized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBAlMYHSIr8Q",
        "outputId": "562076b6-a8bb-4028-b95e-8c0edada2e82"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': array([ 45,  77, 123, 126, 141, 182, 184, 275, 294, 384, 394, 426, 430,\n",
            "       448, 468, 486, 521, 566, 572, 601, 611, 726, 764, 793, 804, 856,\n",
            "       921, 929, 944], dtype=int32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "bonus_dset_tokenized = bonus_ds.map(vectorize_example,num_proc=4)\n",
        "\n",
        "#lets check one vector from the data\n",
        "example = bonus_dset_tokenized[8]\n",
        "\n",
        "pprint(example)\n",
        "\n",
        "#Just checking that the labeling is still ok\n",
        "num_label = example['label']\n",
        "\n",
        "text_label = bonus_dset_tokenized.features['label'].int2str(num_label)\n",
        "\n",
        "print(\"Numerical label:\", num_label)\n",
        "print(\"Corresponding text label:\", text_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK7UCo8XI0PX",
        "outputId": "550dec7b-b505-4362-ff3b-620ddc18f83a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [46,\n",
            "               92,\n",
            "               104,\n",
            "               140,\n",
            "               196,\n",
            "               210,\n",
            "               211,\n",
            "               319,\n",
            "               326,\n",
            "               332,\n",
            "               341,\n",
            "               552,\n",
            "               601,\n",
            "               760,\n",
            "               822,\n",
            "               897,\n",
            "               922],\n",
            " 'label': 0,\n",
            " 'text': 'Fiscal mismanagement of cash costing US Taxpayer billions---cut '\n",
            "         'fraud and waste before cutting funding for Seniors.'}\n",
            "Numerical label: 0\n",
            "Corresponding text label: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bonus_eval_results = bonus_trainer.predict(bonus_dset_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nEgMtqS6JHaU",
        "outputId": "cdd730c2-b198-4fb4-fb6f-53fab4fe1cd7"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(bonus_eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgY-KZspE8JE",
        "outputId": "1da430cd-7982-453a-fcf9-013ca41a328a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PredictionOutput(predictions=array([[ 4.57644403e-01, -3.58707786e-01],\n",
            "       [-7.80374348e-01,  1.11082733e+00],\n",
            "       [ 6.15025684e-02,  1.09371118e-01],\n",
            "       [-5.64188063e-02,  2.46391654e-01],\n",
            "       [-3.46976221e-01,  5.94066620e-01],\n",
            "       [-3.04294229e-01,  5.40958285e-01],\n",
            "       [-2.67233491e-01,  5.01465619e-01],\n",
            "       [ 9.66164470e-03,  1.70603305e-01],\n",
            "       [ 4.85751033e-01, -3.95431906e-01],\n",
            "       [-3.24791729e-01,  5.67284703e-01],\n",
            "       [ 2.08925903e-01, -6.37654215e-02],\n",
            "       [-4.08392310e-01,  6.66656375e-01],\n",
            "       [-3.23123276e-01,  5.65759838e-01],\n",
            "       [ 4.43992972e-01, -3.45990181e-01],\n",
            "       [-1.71758071e-01,  3.86813879e-01],\n",
            "       [ 4.77082551e-01, -3.85391712e-01],\n",
            "       [ 2.32562274e-01, -9.43184122e-02],\n",
            "       [ 1.56319961e-01, -6.96476176e-03],\n",
            "       [-1.33506849e-01,  3.39993924e-01],\n",
            "       [ 2.33054042e-01, -9.90208387e-02],\n",
            "       [-4.17769372e-01,  6.79193795e-01],\n",
            "       [-2.53610790e-01,  4.81323481e-01],\n",
            "       [ 5.74450910e-01, -5.00197232e-01],\n",
            "       [ 1.36240721e-02,  1.64114878e-01],\n",
            "       [ 2.55786002e-01, -1.24153361e-01],\n",
            "       [-8.60338330e-01,  1.20036066e+00],\n",
            "       [ 4.73945439e-01, -3.84618551e-01],\n",
            "       [-4.68413651e-01,  7.36657143e-01],\n",
            "       [-1.11086118e+00,  1.50255394e+00],\n",
            "       [ 7.06746653e-02,  9.74741280e-02],\n",
            "       [-4.63700235e-01,  7.32394874e-01],\n",
            "       [ 5.34676790e-01, -4.52971041e-01],\n",
            "       [-7.01018423e-02,  2.65056580e-01],\n",
            "       [-2.63417840e-01,  4.94092315e-01],\n",
            "       [-3.84840608e-01,  6.37290418e-01],\n",
            "       [ 3.71285141e-01, -2.58416474e-01],\n",
            "       [-4.72745180e-01,  7.45376766e-01],\n",
            "       [-5.03974617e-01,  7.76933908e-01],\n",
            "       [-7.15061575e-02,  2.66239971e-01],\n",
            "       [-6.63265109e-01,  9.66941178e-01],\n",
            "       [-2.78520584e-03,  1.86498255e-01],\n",
            "       [-4.80405450e-01,  7.49379575e-01],\n",
            "       [-1.92640707e-01,  4.10156429e-01],\n",
            "       [ 2.37144113e-01, -9.72439125e-02],\n",
            "       [ 1.03332609e-01,  5.77832609e-02],\n",
            "       [-2.88884342e-02,  2.18572602e-01],\n",
            "       [ 6.67506874e-01, -6.09740555e-01],\n",
            "       [-1.67340621e-01,  3.80609661e-01],\n",
            "       [-4.60303366e-01,  7.26659000e-01],\n",
            "       [-4.63721097e-01,  7.31221974e-01],\n",
            "       [ 4.22674477e-01, -3.22646558e-01],\n",
            "       [-2.67305434e-01,  4.98125762e-01],\n",
            "       [-1.35872915e-01,  3.43320966e-01],\n",
            "       [-1.61904827e-01,  3.72681022e-01],\n",
            "       [ 1.53223991e-01,  7.20066950e-04],\n",
            "       [-2.17618391e-01,  4.40675467e-01],\n",
            "       [-4.24316823e-01,  6.85918987e-01],\n",
            "       [-3.62725556e-01,  6.15012646e-01],\n",
            "       [-3.80551934e-01,  6.33304060e-01],\n",
            "       [ 1.03017613e-01,  6.13174215e-02],\n",
            "       [-2.35271648e-01,  4.61120188e-01],\n",
            "       [-6.95291698e-01,  1.00808668e+00],\n",
            "       [-4.60725129e-01,  7.25516498e-01],\n",
            "       [ 5.35985053e-01, -4.55807477e-01],\n",
            "       [ 3.30463648e-01, -2.08658069e-01],\n",
            "       [ 7.16002285e-01, -6.67838156e-01],\n",
            "       [ 5.05465269e-01, -4.23313439e-01],\n",
            "       [-8.97605717e-03,  1.94029763e-01],\n",
            "       [-3.78756225e-01,  6.28871202e-01],\n",
            "       [ 1.96506128e-01, -5.22330105e-02],\n",
            "       [ 2.64905691e-01, -1.31132394e-01],\n",
            "       [ 2.25571468e-01, -8.47075433e-02],\n",
            "       [-2.85775244e-01,  5.19885182e-01],\n",
            "       [ 8.61915886e-01, -8.40734363e-01],\n",
            "       [ 1.72091082e-01, -2.13283170e-02],\n",
            "       [-4.98968780e-01,  7.72961199e-01],\n",
            "       [ 7.09915608e-02,  9.69803631e-02],\n",
            "       [ 5.85824609e-01, -5.15579522e-01],\n",
            "       [ 2.07312763e-01, -6.57930002e-02],\n",
            "       [ 4.41799521e-01, -3.39657068e-01],\n",
            "       [ 4.11234498e-01, -3.07031512e-01],\n",
            "       [ 5.02791882e-01, -4.16024715e-01],\n",
            "       [-4.03318107e-01,  6.61866903e-01],\n",
            "       [-2.11890712e-01,  4.33494031e-01],\n",
            "       [ 2.49145061e-01, -1.17682621e-01],\n",
            "       [ 5.42067364e-02,  1.14921615e-01],\n",
            "       [ 7.32782125e-01, -6.89323485e-01],\n",
            "       [ 4.12200838e-02,  1.30816832e-01],\n",
            "       [ 2.03976929e-01, -6.11151606e-02],\n",
            "       [ 4.50183451e-02,  1.32469460e-01],\n",
            "       [ 2.69761205e-01, -1.39539227e-01],\n",
            "       [ 2.01609507e-02,  1.55394778e-01],\n",
            "       [ 1.56628937e-02,  1.61870599e-01],\n",
            "       [ 5.66408753e-01, -4.88790393e-01],\n",
            "       [ 5.64607978e-01, -4.87402916e-01],\n",
            "       [ 4.71780717e-01, -3.78892750e-01],\n",
            "       [-5.22955209e-02,  2.44834304e-01],\n",
            "       [-3.75183761e-01,  6.29017651e-01],\n",
            "       [-4.30884808e-02,  2.33006358e-01],\n",
            "       [ 5.77154875e-01, -5.02539754e-01]], dtype=float32), label_ids=array([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1]), metrics={'test_loss': 0.7785675525665283, 'test_accuracy': 0.51, 'test_runtime': 0.0707, 'test_samples_per_second': 1413.504, 'test_steps_per_second': 183.755})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "(Present the results of the evaluation on the out-of-domain test set)\n",
        "\n",
        "### 5.5. Annotated data"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your annotated out-of-domain data here\n",
        "\n",
        "for e in bonus_ds:\n",
        "    text = e['text']\n",
        "    label_num = e['label']\n",
        "    label_name = bonus_ds.features['label'].int2str(label_num)\n",
        "    print(text)\n",
        "    print(f\"Label: {label_num} ({label_name})\")\n",
        "    print('---')"
      ],
      "metadata": {
        "id": "L2YJsiIGeYRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bd3273-4b45-41f2-a8af-a5ccfdff3dec"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Russian Collusion with the Trump Campaign, one of the most successful in history, is a TOTAL HOAX. The Democrats paid for the phony and discredited Dossier which was, along with Comey, McCabe, Strzok and his lover, the lovely Lisa Page, used to begin the Witch Hunt. Disgraceful!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Congresswoman @cathymcmorris of Washington State is an incredible leader who is respected by everyone in Congress. We need her badly in D.C. to keep building on #MAGA. She has my Strong Endorsement!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "To all of those who have asked, I will not be going to the Inauguration on January 20th.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "I applaud and congratulate the U.S. Senate for confirming our GREAT NOMINEE, Judge Brett Kavanaugh, to the United States Supreme Court. Later today, I will sign his Commission of Appointment, and he will be officially sworn in. Very exciting!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Here's to a safe and happy Independence Day for one and all - Enjoy it! --Donald J. Trump\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Melania and I will be appearing on The View tomorrow at 11 a.m. on CBS. Tune in for some great fun!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Many Gang Members and some very bad people are mixed into the Caravan heading to our Southern Border. Please go back, you will not be admitted into the United States unless you go through the legal process. This is an invasion of our Country and our Military is waiting for you!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Miss USA Tara Conner will not be fired - 'I've always been a believer in second chances.' says Donald Trump\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Fiscal mismanagement of cash costing US Taxpayer billions---cut fraud and waste before cutting funding for Seniors.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "MSDNC and FAKE NEWS CNN are going wild trying to protect China!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Mike has my complete &amp; total endorsement. We need him badly in Washington. A great fighter pilot &amp; hero, &amp; a brilliant Annapolis grad, Mike will never let you down. Mail in ballots, &amp; check that they are counted!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Just had a great call with the President of Brazil, @JairBolsonaro. We discussed many subjects including Trade. The relationship between the United States and Brazil has never been Stronger!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Why would @FoxNews put on phony Congresswoman @RepDebDingell. A total waste of airtime!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "While @ BarackObama tries to push gun control http://bit.ly/p27G2K --He still has not answered for Project Gun Runnerhttp://bit.ly/pdtrOw\n",
            "Label: 0 (negative)\n",
            "---\n",
            "All those politicians in Washington and not one good negotiator.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "A total Witch Hunt!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "I got severely criticized by the Fake News Media for being too nice to President Putin. In the Old Days they would call it Diplomacy. If I was loud &amp; vicious, I would have been criticized for being too tough. Remember when they said I was too tough with Chairman Kim? Hypocrites!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Congressman Bill Johnson (@JohnsonLeads) is an incredible fighter for the Great State of Ohio! He’s a proud Veteran and a hard worker who Cares for our Veterans, Supports Small Business, and is Strong on the Border and Second Amendment....\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Republicans and Democrats have both created our economic problems.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "People ask me every day to pose for pictures but the camera never works the first time--they are never prepared or maybe just very nervous!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "from Donald Trump: 'I saw Lady Gaga last night and she was fantastic!'\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'Strive for wholeness and keep your sense of wonder intact.' --Donald J. Trump http://tinyurl.com/pqpfvm\n",
            "Label: 1 (positive)\n",
            "---\n",
            "@johnboehner Debt Ceiling- 'Sometimes no deal is better than a bad deal.'\n",
            "Label: 0 (negative)\n",
            "---\n",
            "America owes our very hard working food supply workers so much as they produce and deliver high quality food for us during this horrible COVID-19. Join me in thanking our Farmers, Ranchers, Processors, Distributors and Stores! @JohnBoozman\n",
            "Label: 1 (positive)\n",
            "---\n",
            "he Fake News Media is going CRAZY! They are totally unhinged and in many ways, after witnessing first hand the damage they do to so many innocent and decent people, I enjoy watching. In 7 years, when I am no longer in office, their ratings will dry up and they will be gone!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "ObamaCare Tragedy Primed to Further Explode the Deficit http://bit.ly/q2iYtw And @ Obama transferred $500 billion from Medicare to fund it!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "If only @ Obama was as focused on balancing the budget as he is on weakening Israel's borders then America would be on the path to solvency.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "@ Obama - Iranian weapons killing Americans in Iraq? What are u doing about this?http://bit.ly/olNqW5\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Olympic Gold Medalist Evan Lysacek just left my office. He is in town and wanted to meet me--he's a fanastic guy and a true champion.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "2016 was AMAZING, but we never had this kind of ENTHUSIASM!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "So China is ordering us to raise the Debt Limit...How low have we as a nation sunk?\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Wishing everyone a wonderful Independence Day weekend. We have a lot to be thankful for.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "I want to thank all of our Great Government officials on the CoronaVirus Task Force who are working around the clock, in response to the CoronaVirus. Continue to check http://CDC.gov for updates, and follow all recommendations that are available....\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'We win in our lives by having a champion's view of each moment.' --Donald J. Trump http://tinyurl.com/pqpfvm\n",
            "Label: 1 (positive)\n",
            "---\n",
            "I never said “give teachers guns” like was stated on Fake News @CNN &amp; @NBC. What I said was to look at the possibility of giving “concealed guns to gun adept teachers with military or special training experience - only the best. 20% of teachers, a lot, would now be able to\n",
            "Label: 0 (negative)\n",
            "---\n",
            "I am honored to be chosen by Gray Line for their NY Ride of Fame Campaign. Today we had the ribbon cutting ceremony in front of Trump Tower.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The Worker in America is doing better than ever before. Celebrate Labor Day!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "This is a terrific day for downtown New York. Trump SoHo is unlike anything else. Be sure to visit this fantastic hotel soon!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "We need a balanced budget Amendment because Congress has no fiscal discipline.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Looking forward to seeing the World Champion Yankees today on opening day!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'The Gang of Six' - yet another unmitigated disaster. ANY DEAL NEEDS TO PEAL OBAMACA. T-E-A.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "We are having very productive calls with the leaders of every sector of the economy who are all-in on getting America back to work, and soon. More to come! #MAGA\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Fake News CNN is looking at big management changes now that they got caught falsely pushing their phony Russian stories. Ratings way down!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Will soon be heading to Wilmington, North Carolina, and then will be going to Battleship North Carolina. Look forward to seeing all of my friends!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Ex-Presidential Pollster Pat Cadell says most voters sick of both parties and their failure.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "@ BarackObama has sold guns to Mexican drug lords while his DOJ erodes our 2nd Amendment rights.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "The European Union makes it impossible for our farmers and workers and companies to do business in Europe (U.S. has a $151 Billion trade deficit), and then they want us to happily defend them through NATO, and nicely pay for it. Just doesn’t work!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "I’m with the TRUCKERS all the way. Thanks for the meeting at the White House with my representatives from the Administration. It is all going to work out well!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "@ BarackObama You have increased discretionary spending by 25% annually and amassed over $4 trillion in debt. T-E-Ahttp://on.wsj.com/onsY3b\n",
            "Label: 0 (negative)\n",
            "---\n",
            "America's debt is greater than our GDP. Time for new thinking.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "America’s police officers have earned the everlasting gratitude of our Nation. In moments of danger &amp; despair you are the reason we never lose hope – because there are men &amp; women in uniform who face down evil &amp; stand for all that is GOOD and JUST and DECENT and RIGHT! #IACP2018\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'China is our enemy--they want to destroy us' -- Redstate Interview\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Our thoughts and prayers remain with Bret Michaels and his family and for his speedy recovery.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The Russia Hoax is the biggest political scandal in American history. Treason!!! Lets see how it ends????\n",
            "Label: 0 (negative)\n",
            "---\n",
            "- Wishing a Happy Father's Day to all the Dad's out there - YOU are a champion today and everyday! http://tinyurl.com/kn95ju\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'My persona will never be that of a wallflower - I‚Äôd rather build walls than cling to them' --Donald J. Trump\n",
            "Label: 0 (negative)\n",
            "---\n",
            "My Approval Rating in the Republican Party is 95%, a Record. Thank you!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Disappointed in GOP and Dems---Giving Obama power to raise the debt limit next year is a mistake.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Some of the Fake News Media likes to say that I am not totally engaged in healthcare. Wrong, I know the subject well &amp; want victory for U.S.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Thank you to Democrat Assemblyman Dov Hikind of New York for your very gracious remarks on @foxandfriends for our deporting a longtime resident Nazi back to Germany! Others worked on this for decades.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The original Apprentice returns with a two hour premiere on Thursday September 16th. Looking forward to a fantastic season!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The “Surrender Caucus” within the Republican Party will go down in infamy as weak and ineffective “guardians” of our Nation, who were willing to accept the certification of fraudulent presidential numbers!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "I don’t care what the political ramifications are, our immigration laws and border security have been a complete and total disaster for decades, and there is no way that the Democrats will allow it to be fixed without a Government Shutdown...\n",
            "Label: 0 (negative)\n",
            "---\n",
            "We are marshalling the full power of government and society to achieve victory over the virus. Together, we will endure, we will prevail, and we will WIN! #CARESAct\n",
            "Label: 1 (positive)\n",
            "---\n",
            "I told you so! The European Union just slapped a Five Billion Dollar fine on one of our great companies, Google. They truly have taken advantage of the U.S., but not for long!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Thanks to all for your thoughtful birthday wishes ‚Äì Donald Trump\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The Fake News doesn’t show real polls. Lamestream Media is totally CORRUPT, the Enemy of the People!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Great day in North Carolina where Republicans will do very well!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "We're all very happy to hear of Bret Michael's progress and send our best wishes for his full recovery.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "What is better advice- 'The Art of the Deal' or 'Rules for Radicals'? I know which one @ BarackObama prefers.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "If Republican Senators are unable to pass what they are working on now, they should immediately REPEAL, and then REPLACE at a later date!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "'Be aware of things that seem inexplicable because they can be a big step towards innovation.' --Donald J. Trump http://tinyurl.com/pqpfvm\n",
            "Label: 1 (positive)\n",
            "---\n",
            "'Always know you could be on the precipice of something great.' --Donald J. Trump http://tinyurl.com/pqpfvm\n",
            "Label: 1 (positive)\n",
            "---\n",
            "This Russian connection non-sense is merely an attempt to cover-up the many mistakes made in Hillary Clinton's losing campaign.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Don’t allow RIGGED ELECTIONS!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "We could use the Balanced Budget Amendment--Politicians don't have the will to cut spending\n",
            "Label: 0 (negative)\n",
            "---\n",
            "No taxes the only good thing about DC Debt Deal.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Where the hell is the Durham Report? They spied on my campaign, colluded with Russia (and others), and got caught. Read the Horowitz Reports about Comey &amp; McCabe. Even the Fake News @nytimes said “bad”. They tried it all, and failed, so now they are trying to steal the election!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Spent a beautiful weekend golfing at Trump National Golf Club Westchester and Trump National Golf Club Bedminster.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "A very good sign is that empty hospital beds are becoming more and more prevalent. We deployed 418 Doctors, Nurses and Respiratory Therapists from the hospital ship Comfort and the Javits Convention Center to hospitals in NYC &amp; State. Have more bed capacity than was needed. Good!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Unfortunately@BarackObama's continued attack on the US $ will lead to ever rising gas prices at the pump and lots of other really bad things\n",
            "Label: 0 (negative)\n",
            "---\n",
            "America is the Greatest Country in the world. We have the best scientists, doctors, nurses and health care professionals. They are amazing people who do phenomenal things every day....\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Every dollar @ BarackObama spends costs $1.40 with interest borrowed from China on our children and grandchildren's backs. CUT-CAP-BALANCE!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Melania and I saw American Idiot on Broadway last night and it was great. An amazing theatrical experience!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Massive crowd inside and outside the Allen County War Memorial Coliseum in Fort Wayne, Indiana! Thank you for joining us tonight - and make sure you get out and https://t.co/0pWiwCHGbh tomorrow!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "It was my great honor to address the International Association of Chiefs of Police Annual Convention in Orlando, Florida. Thank you! #IACP2018 #LESM\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Congrats to winners from around the world who entered the 'Think Like A Champion' signed book/keychain contest! http://tinyurl.com/pqpfvm\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Wow! The NSA has deleted 685 million phone calls and text messages. Privacy violations? They blame technical irregularities. Such a disgrace. The Witch Hunt continues!\n",
            "Label: 0 (negative)\n",
            "---\n",
            "George Steinbrenner was a great friend and a true legend. There will never be anyone like him in New York. We've lost a truly great man.\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Great job being done by the @ VP and the CoronaVirus Task Force. Thank you!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The “Justice” Department and the FBI have done nothing about the 2020 Presidential Election Voter Fraud, the biggest SCAM in our nation’s history, despite overwhelming evidence. They should be ashamed. History will remember. Never give up. See everyone in D.C. on January 6th.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Congratulations to Bryan Steil on a wonderful win last night. You will be replacing a great guy in Paul Ryan, and your win in November will make the entire State of Wisconsin very proud. You have my complete and total Endorsement!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "Presidential Approval numbers are very good - strong economy, military and just about everything else. Better numbers than Obama at this point, by far. We are winning on just about every front and for that reason there will not be a Blue Wave, but there might be a Red Wave!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "The United States made some of the worst Trade Deals in world history.Why should we continue these deals with countries that do not help us?\n",
            "Label: 0 (negative)\n",
            "---\n",
            "Wishing you and yours a very Happy and Bountiful Thanksgiving!\n",
            "Label: 1 (positive)\n",
            "---\n",
            "“The highest level of bias I’ve ever witnessed in any law enforcement officer.” Trey Gowdy on the FBI’s own, Peter Strzok. Also remember that they all worked for Slippery James Comey and that Comey is best friends with Robert Mueller. A really sick deal, isn’t it?\n",
            "Label: 0 (negative)\n",
            "---\n",
            "@johnboehner Message for House GOP 'The worst thing you can possibly do in a deal is seem desperate to make it.' - The Art of the Deal.\n",
            "Label: 0 (negative)\n",
            "---\n",
            "GREAT news this week regarding the Keystone XL Pipeline – moving forward with fantastic paying CONSTRUCTION jobs for hardworking Americans. Promises Made, Promises Kept! #MAGA\n",
            "Label: 1 (positive)\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}