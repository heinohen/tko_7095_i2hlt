{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python function that takes a dataset name as an argument, loads the dataset, and calculates and prints out the following information about the dataset:\n",
        "\n",
        "    The description of the dataset (use load_dataset_builder)\n",
        "    Relative sizes of the subsets of the dataset (e.g. 'train', 'validation', and 'test') in terms of examples (rows). For example: \"train: 50%, validation: 25%, test: 25%\"\n",
        "    Distribution of labels in the 'train' subset of the dataset, using the names of the labels. For example: \"positive: 53%, negative: 47%\"\n",
        "\n",
        "(You can assume that the function will only be called with the names of datasets representing text classification corpora.)\n",
        "\n",
        "Apply this function to the following datasets: 'emotion', 'rotten_tomatoes', 'snli', 'sst2', 'emo'.\n"
      ],
      "metadata": {
        "id": "ed5FsUjqCsu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Zlg7lBbuDQlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbfb4f2-8574-4af2-ba50-300f5ca973bd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtXqnPyRCnF6",
        "outputId": "2207ad89-85ce-48c0-95fa-d898e6291b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************\n",
            "Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.\n",
            "\n",
            "train: 80% \tvalidation: 10% \ttest: 10% \t\n",
            "sadness: 29% \tjoy: 34% \tlove: 8% \tanger: 13% \tfear: 12% \tsurprise: 4% \t\n",
            "********************\n",
            "********************\n",
            "No description found for dataset rotten_tomatoes\n",
            "train: 80% \tvalidation: 10% \ttest: 10% \t\n",
            "neg: 50% \tpos: 50% \t\n",
            "********************\n",
            "********************\n",
            "No description found for dataset snli\n",
            "test: 2% \tvalidation: 2% \ttrain: 96% \t\n",
            "entailment: 33% \tneutral: 33% \tcontradiction: 33% \t\n",
            "********************\n",
            "********************\n",
            "No description found for dataset sst2\n",
            "train: 96% \tvalidation: 1% \ttest: 3% \t\n",
            "negative: 44% \tpositive: 56% \t\n",
            "********************\n",
            "********************\n",
            "In this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others.\n",
            "\n",
            "train: 85% \ttest: 15% \t\n",
            "others: 50% \thappy: 14% \tsad: 18% \tangry: 18% \t\n",
            "********************\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import datasets\n",
        "from datasets import load_dataset_builder\n",
        "from datasets import load_dataset\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        "\n",
        "# Actual func\n",
        "def extractor(x):\n",
        "\n",
        "  # The description of the dataset (use load_dataset_builder)\n",
        "  ds_builder = load_dataset_builder(x, trust_remote_code=True)\n",
        "  description = ds_builder.info.description\n",
        "\n",
        "\n",
        "  # Relative sizes of the subsets of the dataset\n",
        "  dataset = load_dataset(x, trust_remote_code=True)\n",
        "  subsets = dict(dataset.num_rows)\n",
        "  allrows = sum(subsets.values())\n",
        "  st = \"\"\n",
        "  for k,v in subsets.items():\n",
        "    st += f'{k}: {round((v * 100) / allrows)}% \\t'\n",
        "\n",
        "\n",
        "  #Distribution of labels in the 'train' subset of the dataset, using the names of the labels. For example: \"positive: 53%, negative: 47%\"\n",
        "  only_train = load_dataset(x, split = 'train', trust_remote_code=True)\n",
        "  label_list = list(only_train.features['label'].names)\n",
        "  label_counts = [0]*len(label_list)\n",
        "\n",
        "  for entry in only_train:\n",
        "    # entry['label'] <- this extracts only the assigned label\n",
        "    label_counts[entry['label']] += 1 # accumulate the index of said\n",
        "\n",
        "  label_allcounts = sum(label_counts)\n",
        "  LABELS = dict()\n",
        "  for i, label in enumerate(label_list):\n",
        "    LABELS[label] = label_counts[i]\n",
        "\n",
        "  sol = \"\"\n",
        "  for k,v in LABELS.items():\n",
        "    sol += f'{k}: {round((v * 100) / label_allcounts)}% \\t'\n",
        "\n",
        "\n",
        "  # PRINTS\n",
        "  print(\"*\"*20)\n",
        "  if len(description) == 0:\n",
        "    print(f'No description found for dataset {x}')\n",
        "  else:\n",
        "    print(description) # A\n",
        "\n",
        "  print(st) # B\n",
        "  print(sol) # C\n",
        "  print(\"*\"*20)\n",
        "\n",
        "# Function calls\n",
        "DATASETS = ['emotion', 'rotten_tomatoes', 'snli', 'sst2', 'emo']\n",
        "\n",
        "for x in DATASETS:\n",
        "  extractor(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What patterns can you notice in the relative sizes of the subsets? Can you tell why this might be?"
      ],
      "metadata": {
        "id": "Aa4u763vSbs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the datasets are 80/10/10 split or more.\n",
        "\n",
        "Amount of training data needed is vastly bigger to train the model so that it learns more. Some datasets provide additional validation data to validate the model during training."
      ],
      "metadata": {
        "id": "FSuyeTpJBPT_"
      }
    }
  ]
}