{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvmCTzUjwIrrCNW4XKP4xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heinohen/tko_7095_i2hlt/blob/main/week3_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zsa2vO18Ve-O"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q transformers[torch] datasets evaluate optuna plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint # Pretty print\n",
        "import datasets\n",
        "from google.colab import userdata\n",
        "userdata.get('hf') # hugging face secret\n",
        "dset = datasets.load_dataset('imdb') # loads dataset\n",
        "print(dset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz5LFG6DVoqU",
        "outputId": "8f1b5f99-e99e-47ae-e4f6-c170ae8c7728"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset = dset.shuffle() # Like i thought in week2 ex, this should be done...\n",
        "del dset['unsupervised']"
      ],
      "metadata": {
        "id": "uN_-2GdcWQCU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction\n",
        "\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary = True, max_features = 20000) # As in ex\n",
        "\n",
        "texts = [ex['text'] for ex in dset['train']] # Generates a list of all texts from the 'train' category of data\n",
        "vectorizer.fit(texts) # Learn a vocabulary dictionary of all tokens in the raw documents."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "gK_zBDFPWcYC",
        "outputId": "5c58a2e7-3029-4c55-b91b-b0e0cdfcc2c0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from course\n",
        "\n",
        "def vectorize_example(ex) -> dict:\n",
        "  vectorized = vectorizer.transform([ex['text']]) # Transform documents to document-term matrix.\n",
        "  non_zero_features = vectorized.nonzero()[1] # This is from torch 'nonzero' returns a 2-D tensor where each row is the index for a nonzero value.\n",
        "  non_zero_features += 1 #feature index 0 will have a special meaning\n",
        "                         # so let us not produce it by adding +1 to everything\n",
        "  return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized = vectorize_example(dset['train'][0])"
      ],
      "metadata": {
        "id": "AZllAhoIXeCB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from course\n",
        "\n",
        "index_to_word = dict((k,v) for (v,k) in vectorizer.vocabulary_.items()) # Inverses the vocabulary key and value\n",
        "words = []\n",
        "for i in vectorized['input_ids']:\n",
        "  words.append(index_to_word[i-1]) # move back to zero-index start\n",
        "\n",
        "pprint(', '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_1E_x94YukT",
        "outputId": "20100756-5d57-4a21-aeac-335b4e3c49a7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('about, acting, actors, all, american, and, are, arguing, at, between, both, '\n",
            " 'but, by, character, classic, compare, complicated, crimes, english, '\n",
            " 'enjoying, evaluate, fact, far, funny, good, hand, have, how, if, is, it, '\n",
            " 'judge, just, language, like, looks, main, maybe, more, no, not, on, or, '\n",
            " 'other, people, plot, plots, police, prefer, prime, quite, really, relation, '\n",
            " 'series, similarities, simple, spirit, spot, superficial, suspect, than, '\n",
            " 'that, the, there, they, thing, think, this, to, too, violent, way, we, weak, '\n",
            " 'weirdo, writing')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "\n",
        "# Multiprocessing significantly speeds up processing by parallelizing processes on the CPU.\n",
        "# Set the num_proc parameter in map() to set the number of processes to use:\n",
        "\n",
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dset_tokenized = dset.map(vectorize_example,num_proc=4)\n",
        "pprint(dset_tokenized[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90w_m17FZ9_5",
        "outputId": "e94741dd-b376-4695-9332-faeb2a14941c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [309,\n",
            "               419,\n",
            "               430,\n",
            "               727,\n",
            "               826,\n",
            "               887,\n",
            "               1115,\n",
            "               1129,\n",
            "               1300,\n",
            "               1901,\n",
            "               2248,\n",
            "               2604,\n",
            "               2625,\n",
            "               3053,\n",
            "               3337,\n",
            "               3676,\n",
            "               3726,\n",
            "               4299,\n",
            "               6087,\n",
            "               6104,\n",
            "               6294,\n",
            "               6584,\n",
            "               6663,\n",
            "               7406,\n",
            "               7801,\n",
            "               8190,\n",
            "               8322,\n",
            "               8780,\n",
            "               8929,\n",
            "               9602,\n",
            "               9630,\n",
            "               9846,\n",
            "               9890,\n",
            "               10217,\n",
            "               10475,\n",
            "               10644,\n",
            "               10871,\n",
            "               11137,\n",
            "               11681,\n",
            "               12134,\n",
            "               12202,\n",
            "               12437,\n",
            "               12504,\n",
            "               12564,\n",
            "               13028,\n",
            "               13359,\n",
            "               13362,\n",
            "               13426,\n",
            "               13634,\n",
            "               13739,\n",
            "               14131,\n",
            "               14349,\n",
            "               14571,\n",
            "               15790,\n",
            "               16137,\n",
            "               16145,\n",
            "               16734,\n",
            "               16788,\n",
            "               17378,\n",
            "               17471,\n",
            "               17885,\n",
            "               17893,\n",
            "               17897,\n",
            "               17929,\n",
            "               17944,\n",
            "               17951,\n",
            "               17954,\n",
            "               17968,\n",
            "               18115,\n",
            "               18166,\n",
            "               19196,\n",
            "               19440,\n",
            "               19446,\n",
            "               19447,\n",
            "               19500,\n",
            "               19845],\n",
            " 'label': 1,\n",
            " 'text': 'There is no relation at all between Fortier and Profiler but the '\n",
            "         'fact that both are police series about violent crimes. Profiler '\n",
            "         'looks crispy, Fortier looks classic. Profiler plots are quite '\n",
            "         \"simple. Fortier's plot are far more complicated... Fortier looks \"\n",
            "         'more like Prime Suspect, if we have to spot similarities... The main '\n",
            "         'character is weak and weirdo, but have \"clairvoyance\". People like '\n",
            "         'to compare, to judge, to evaluate. How about just enjoying? Funny '\n",
            "         'thing too, people writing Fortier looks American but, on the other '\n",
            "         \"hand, arguing they prefer American series (!!!). Maybe it's the \"\n",
            "         'language, or the spirit, but I think this series is more English '\n",
            "         'than American. By the way, the actors are really good and funny. The '\n",
            "         'acting is not superficial at all...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collator(list_of_examples):\n",
        "  batch = {'labels':torch.tensor(list(ex['label'] for ex in list_of_examples))} # Labels in to a single tensor\n",
        "  tensors = []\n",
        "  max_len = max(len(example['input_ids']) for example in list_of_examples) # Get the length of longest input\n",
        "  # To build a tensor\n",
        "  for e in list_of_examples:\n",
        "    ids = torch.tensor(e['input_ids']) # Pick the input ids\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
        "    # pad(input, (left, right))\n",
        "    padded = torch.nn.functional.pad(ids, (0, max_len - ids.shape[0]))\n",
        "    tensors.append(padded)\n",
        "  # https://pytorch.org/docs/stable/generated/torch.vstack.html\n",
        "  batch['input_ids'] = torch.vstack(tensors) # Stack tensors in sequence vertically (row wise).\n",
        "  return batch"
      ],
      "metadata": {
        "id": "MoSZVEGObUrE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This is quite clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "        # Output layer: hidden size x output size\n",
        "        self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "\n",
        "    # The computation of the model is put into the forward() function\n",
        "    # it receives a batch of data and optionally the correct `labels`\n",
        "    #\n",
        "    # If given `labels` it returns (loss,output)\n",
        "    # if not, then it returns (output,)\n",
        "    def forward(self,input_ids,labels=None): #nevermind the attention_mask, its time will come, data collator insists on adding it\n",
        "        #1) sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "\n",
        "        #2) apply non-linearity\n",
        "        # (batch,embedding_dim) -> (batch,embedding_dim)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> commented out\n",
        "        ####projected=torch.tanh(embedded_summed) #Note how non-linearity is applied here and not when configuring the layer in __init__()\n",
        "\n",
        "        #3) and now apply the upper, output layer of the network\n",
        "        # (batch,embedding_dim) -> (batch, num_of_classes i.e. 2 in our case)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> base it off embedded_summed\n",
        "        ##### OLD: logits=self.output(projected)\n",
        "        logits=self.output(embedded_summed)\n",
        "\n",
        "        # ...and that's all there is to it!\n",
        "\n",
        "        #print(\"input_ids.shape\",input_ids.shape)\n",
        "        #print(\"embedded.shape\",embedded.shape)\n",
        "        #print(\"embedded_summed.shape\",embedded_summed.shape)\n",
        "        #print(\"projected.shape\",projected.shape)\n",
        "        #print(\"logits.shape\",logits.shape)\n",
        "\n",
        "        # We have labels, so we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)\n",
        "\n",
        "# Configure the model:\n",
        "#   these parameters are used in the model's __init__()\n",
        "\n",
        "\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=20,nlabels=2)\n"
      ],
      "metadata": {
        "id": "tZqww87RdOzV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n"
      ],
      "metadata": {
        "id": "jfRZxjMkgtnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "mlp = MLP(mlp_config)\n",
        "fake_batch = collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call input_ids and labels as parameters to the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxJVYFRxgxQC",
        "outputId": "96d1271d-8838-49b3-ca3c-899d82255475"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7865, grad_fn=<NllLossBackward0>),\n",
              " tensor([[0.1887, 0.0079],\n",
              "         [0.1886, 0.0119]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training arguments"
      ],
      "metadata": {
        "id": "xWenXJmyivJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps.\n",
        "    logging_strategy=\"steps\", #  Logging is done every logging_steps.\n",
        "    eval_steps=500, # Number of update steps between two evaluations if evaluation_strategy=\"steps\".\n",
        "    # Will default to the same value as logging_steps if not set.\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    logging_steps=500, #  Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    # Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    # float, optional, defaults to 5e-5) â€” The initial learning rate.\n",
        "    max_steps=20000, #  (int, optional, defaults to -1)\n",
        "    # If set to a positive number, the total number of training steps to perform.\n",
        "    # Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted)\n",
        "\n",
        "    # until max_steps is reached.\n",
        "    #num_train_epochs=5.0,\n",
        "    load_best_model_at_end=True, # Whether or not to load the best model found during training at the end of training.\n",
        "    # When this option is enabled, the best checkpoint will always be saved.\n",
        "    per_device_train_batch_size = 64\n",
        ")\n",
        "\n",
        "pprint(trainer_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l890LD2TixjA",
        "outputId": "257716f5-af0f-45e6-827e-ce4e0686fa8f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/Mar30_15-11-21_a14651e969e9,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ajIPSj-2vto8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute accuracy\n"
      ],
      "metadata": {
        "id": "7wuGpSqgolE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Evaluate is a library that makes evaluating and comparing models\n",
        "# and reporting their performance easier and more standardized.\n",
        "# https://pypi.org/project/evaluate/\n",
        "\n",
        "accuracy = evaluate.load('accuracy')\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "  outputs, labels = outputs_and_labels\n",
        "  preds = np.argmax(outputs, axis = -1) # Returns the indices of the maximum values along an axis.\n",
        "  # https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
        "  return accuracy.compute(predictions = preds, references = labels)"
      ],
      "metadata": {
        "id": "UusXPpreonYY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a new model\n"
      ],
      "metadata": {
        "id": "fnSBkgBlpW4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP(mlp_config)\n",
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "# ( early_stopping_patience: int = 1, early_stopping_threshold: Optional = 0.0 )\n",
        "#  â€” Use with metric_for_best_model to stop training when the specified metric worsens\n",
        "# for early_stopping_patience evaluation calls.\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model = mlp,\n",
        "    args = trainer_args,\n",
        "    train_dataset = dset_tokenized['train'],\n",
        "    eval_dataset = dset_tokenized['test'].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics = compute_accuracy,\n",
        "    data_collator = collator,\n",
        "    callbacks = [early_stopping]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "hM7O6g1fpYxQ",
        "outputId": "93b682f7-8212-4a4f-ca4a-fadaed828073"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4500' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4500/20000 01:19 < 04:32, 56.89 it/s, Epoch 11/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.522600</td>\n",
              "      <td>0.414814</td>\n",
              "      <td>0.852000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.316700</td>\n",
              "      <td>0.326775</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.299593</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.292867</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.167200</td>\n",
              "      <td>0.294402</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.140800</td>\n",
              "      <td>0.300209</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.127100</td>\n",
              "      <td>0.310201</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.107000</td>\n",
              "      <td>0.323284</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.095200</td>\n",
              "      <td>0.336304</td>\n",
              "      <td>0.866000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory mlp_checkpoints/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-4000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-4500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4500, training_loss=0.2123383526272244, metrics={'train_runtime': 79.0895, 'train_samples_per_second': 16184.204, 'train_steps_per_second': 252.878, 'total_flos': 28613549664.0, 'train_loss': 0.2123383526272244, 'epoch': 11.51})"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reporting blog ðŸ˜¸"
      ],
      "metadata": {
        "id": "Of8x8H9lr7Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEARNING RATE**"
      ],
      "metadata": {
        "id": "z_-Msf7gvlWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "learning_rate=0.0001,\n",
        "# All other unchanged\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=4500, training_loss=0.18218419308132597, metrics={'train_runtime': 133.83, 'train_samples_per_second': 19128.74, 'train_steps_per_second': 149.443, 'total_flos': 61814021472.0, 'train_loss': 0.18218419308132597, 'epoch': 22.96})\n"
      ],
      "metadata": {
        "id": "Prh-Gx8esgtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "learning_rate=1e-05,\n",
        ",\n",
        "# All other unchanged\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "TrainOutput(global_step=20000, training_loss=0.33597693824768066, metrics={'train_runtime': 625.5423, 'train_samples_per_second': 4092.449, 'train_steps_per_second': 31.972, 'total_flos': 274545274752.0, 'train_loss': 0.33597693824768066, 'epoch': 102.04})\n",
        "\n"
      ],
      "metadata": {
        "id": "scYZ2hXvvwCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "learning_rate=1e-05,\n",
        ",\n",
        "# All other unchanged\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=20000, training_loss=0.6240427291870118, metrics={'train_runtime': 616.7456, 'train_samples_per_second': 4150.82, 'train_steps_per_second': 32.428, 'total_flos': 274545274752.0, 'train_loss': 0.6240427291870118, 'epoch': 102.04})"
      ],
      "metadata": {
        "id": "lJ2N9ktH0Roi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NUM_TRAIN_EPOCHS**\n",
        "\n",
        "An epoch in machine learning means one complete pass of the training dataset through the algorithm"
      ],
      "metadata": {
        "id": "aYuKskJv1J8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "num_train_epochs = 1,\n",
        ",\n",
        "# All other unchanged\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=196, training_loss=0.6293549051090163, metrics={'train_runtime': 5.3816, 'train_samples_per_second': 4645.454, 'train_steps_per_second': 36.42, 'total_flos': 2702724192.0, 'train_loss': 0.6293549051090163, 'epoch': 1.0})"
      ],
      "metadata": {
        "id": "4zYQCsKW1erV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "num_train_epochs = 3,\n",
        ",\n",
        "# All other unchanged\n",
        ")\n",
        "TrainOutput(global_step=588, training_loss=0.5115975944363341, metrics={'train_runtime': 17.3097, 'train_samples_per_second': 4332.835, 'train_steps_per_second': 33.969, 'total_flos': 8088105312.0, 'train_loss': 0.5115975944363341, 'epoch': 3.0})"
      ],
      "metadata": {
        "id": "LwEkaGMW1UvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "num_train_epochs = 5,\n",
        ",\n",
        "# All other unchanged\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=980, training_loss=0.43474208286830357, metrics={'train_runtime': 29.7027, 'train_samples_per_second': 4208.372, 'train_steps_per_second': 32.994, 'total_flos': 13483141056.0, 'train_loss': 0.43474208286830357, 'epoch': 5.0})\n",
        "\n"
      ],
      "metadata": {
        "id": "P_VVNmyL1ksV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PER_DEVICE_BATCH**\n",
        "\n"
      ],
      "metadata": {
        "id": "dr_F4Z3x5YXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TrainingArguments(\n",
        "\n",
        "per_device_batch = 8,\n",
        "All other unchanged\n",
        "\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=8500, training_loss=0.26540270457548254, metrics={'train_runtime': 58.3357, 'train_samples_per_second': 2742.744, 'train_steps_per_second': 342.843, 'total_flos': 4506209568.0, 'train_loss': 0.26540270457548254, 'epoch': 2.72})"
      ],
      "metadata": {
        "id": "PZab7Nv65mhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "per_device_batch = 32,\n",
        "# All other unchanged\n",
        "\n",
        ")\n",
        "\n",
        "TrainOutput(global_step=5000, training_loss=0.23753613052368164, metrics={'train_runtime': 59.7578, 'train_samples_per_second': 10709.907, 'train_steps_per_second': 334.685, 'total_flos': 14289180192.0, 'train_loss': 0.23753613052368164, 'epoch': 6.39})"
      ],
      "metadata": {
        "id": "d_cSLvMf5oFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TrainingArguments(\n",
        "\n",
        "per_device_batch = 64,\n",
        "# All other unchanged\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "TrainOutput(global_step=4500, training_loss=0.2123383526272244, metrics={'train_runtime': 79.0895, 'train_samples_per_second': 16184.204, 'train_steps_per_second': 252.878, 'total_flos': 28613549664.0, 'train_loss': 0.2123383526272244, 'epoch': 11.51})"
      ],
      "metadata": {
        "id": "GgFunYxN6_FA"
      }
    }
  ]
}